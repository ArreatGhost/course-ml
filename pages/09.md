---
presentation:
  margin: 0
  center: false
  transition: "convex"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"
@import "https://cdn.bootcdn.net/ajax/libs/jquery/3.5.0/jquery.js"

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## k-近邻法

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide vertical=true data-notes="" -->

##### 大纲

---

@import "../vega/outline.json" {as="vega" .top-2}

<!-- slide data-notes="" -->

##### <span style="font-weight:900">k-</span>近邻法

---

基本假设：{==相似的样本属于相同的类别==}

如何刻画相似？{==距离函数==}：$\dist(\cdot, \cdot): \Xcal \times \Xcal \mapsto \Rbb^+$

<div class="top2"></div>

输入：$D = \{ (\xv_i, y_i) \}_{i \in [m]} \in (\Xcal \times \Ycal)^m$，近邻数$k$，待预测样本$\xv$

<div class="top-3"></div>

输出：$\xv$的类别$y$

1. {==寻找近邻==}：求解$N_k(\xv) \subseteq D$使得$|N_k(\xv)| = k$，且对$\forall (\xv', y') \in D \setminus N_k(\xv)$，有$\dist(\xv, \xv') \geq \max_{\zv \in N_k(\xv)} \dist (\xv, \zv)$
2. {==多数投票==}：输出$\mode(\{ y'': (\xv'', y'') \in N_k(\xv) \})$，其中$\mode(\cdot)$表示众数

<div class="top4"></div>

我的批注 近邻法没有{==显式==}的学习过程

<!-- slide vertical=true data-notes="" -->

##### 空间划分 <span style="font-weight:900">1-</span>近邻

---

@import "../tikz/knn.svg" {.center .width50 .top5}

<!-- slide vertical data-notes="" -->

##### 超参设置

---

近邻数$k$：取值范围$[m] \wedge \{2 \Zbb + 1\}$

- {==奇数==}可保证取众数时不会出现{==打平==}的情况，zyzzj 常委都是奇数位
- 越小越容易过拟合，越大越容易欠拟合，实践中多通过交叉验证选取

<div class="top2"></div>

距离函数：

- 闵可夫斯基距离$\dist(\xv, \zv) = \| \xv - \zv \|_p$，由$\ell_p$范数诱导出
- 马氏距离$\dist_\Mv (\xv, \zv) \triangleq \sqrt{(\xv - \zv)^\top \Mv (\xv - \zv)}$，当$\Mv$为以$w_1, \ldots, w_d$为对角元素的对角阵时，即为加权平方距离$\sqrt{\sum_{j \in [d]} w_j (x_j - z_j)^2}$

<div class="top2"></div>

{==度量学习==} (metric learning)：学一个更好的距离函数，以马氏距离为例，记$\Mcal$/$\Ccal$分别为同类/异类样本对构成的集合

$$
\begin{align*}
    \quad \min_\Mv & \sum_{(\xv_i, \xv_j) \in \Mcal} \dist_\Mv(\xv_i, \xv_j), \quad \st \sum_{(\xv_i, \xv_j) \in \Ccal} \dist_\Mv(\xv_i, \xv_j) \ge 1, ~ \Mv \succeq \zerov
\end{align*}
$$

<!-- slide vertical data-notes="" -->

##### 优劣

---

优点

- 简单，全方位的
- 无训练过程，只需存下数据，惰性学习 (lazy learning)
- 样本极少时也能用
- 特征空间维度不高时效果很好
- {==一致性==}：若贝叶斯最优分类器的错误率$R^\star = 0$，k-近邻也能渐进达到

<div class="top4"></div>

缺点

- 预测很慢，要计算待预测样本与训练集中所有样本的距离
- {==维度灾难==}：高维空间中的距离会失效，k-近邻效果很差

<!-- slide data-notes="" -->

##### 理论分析 <span style="font-weight:900">1-</span>近邻法

---

定理：设$(\Xcal, \dist(\cdot, \cdot))$是{==可分度量空间==}，标记集合$\Ycal = \{ 1, -1 \}$，贝叶斯最优分类器$h^\star(\xv) = \mathop{\arg\max}_{y \in \Ycal} \Pbb(y|\xv)$的错误率为$R^\star$，数据集$D = \{ (\xv_i, y_i) \}_{i \in [m]} \in (\Xcal \times \Ycal)^m$，1-近邻法的泛化错误率为$R_m$，其中下标$m$表示样本数，记$R = \lim_{m \rightarrow \infty} R_m$为渐进泛化错误率，则$R^\star \le R \le 2 R^\star (1 - R^\star)${==几乎必然==}成立

一些说明：

- 下界是显然的，不可能比贝叶斯最优分类器更好
- 上界表明 1-近邻法最坏情况下错误率不超过贝叶斯最优分类器的 2 倍
- 当$R^\star = 0$ (两类完全分开) 或$R^\star = 1/2$ (两类完全重叠) 时，界是{==紧==}的

<!-- slide vertical=true data-notes="" -->

##### 名词解释 空间

---

空间 = 集合 + 结构

- 集合：把需要研究元素放到一起
- 结构：描述元素必须遵循的规则

<div class="top2"></div>

代数结构：加法和数乘的 8 条公理，线性空间 (向量空间)

<div class="top-2"></div>

线性空间无法计算{==元素间的距离==}，从而无法谈{==序列的收敛性==}

引入几何结构刻画元素间的亲疏远近，由弱到强依次有

- 邻域 (开集族)：拓扑空间
- 距离函数$\dist(\cdot, \cdot): \Xcal \times \Xcal \mapsto \Rbb^+$：度量空间
- 代数结构 + $\|\cdot\|$：赋范空间，$\dist(\xv, \zv) = \| \xv - \zv \|$
- 代数结构 + $\langle \cdot, \cdot \rangle$：内积空间，$\| \xv - \zv \|^2 = \langle \xv, \xv \rangle - 2 \langle \xv, \zv \rangle + \langle \zv, \zv \rangle$
- 范数可通过内积定义，故内积空间也是赋范空间，范数对距离同理

<!-- slide vertical=true data-notes="" -->

##### 名词解释 空间

---

{==完备性==} (completeness)：对极限运算封闭，柯西序列能收敛

- $\Qbb$不完备，有理数序列$(1 + 1 / n)^n \rightarrow e$，$\Qbb$的完备化是$\Rbb$

<div class="top1"></div>

@import "../dot/space.dot" {.center}

<div class="top-2"></div>

{==可分性==} (separability)：具有可数稠密子集

- 若度量空间$\Xcal$的子集$\Mcal$满足对$\forall x \in \Xcal$，$x$的任意邻域与$\Mcal$交集非空，则称$\Mcal$在$\Xcal$中{==稠密==}，$\Qbb$在$\Rbb$中稠密，$\Qbb$可数，因此$\Rbb$可分
- 可分性限制了空间的复杂度，即便空间中的元素可能是不可数的，但每个元素都可以被一个可数集中的元素无限逼近，而可数集更好处理

<!-- slide vertical=true data-notes="" -->

##### 名词解释 几乎必然

---

几乎必然 (almost surely, a.s.) 成立也称{==以概率 1 成立==}

- 当随机事件的样本空间{==有限==}时，等价于{==必然成立==}
- 当随机事件的样本空间{==无限==}时，两者不等价

<div class="top2"></div>

在$[0,1]$上随机挑一个数$x$，$x$几乎必然不等于$0.5$

$$
\begin{align*}
    \quad \Pbb(x = 0.5) = \int_{0.5}^{0.5} 1 ~ \diff x = 0, \quad \Pbb(x \ne 0.5) = 1 - \Pbb(x = 0.5) = 1
\end{align*}
$$

<div class="top-3"></div>

但$x = 0.5$是有可能的，更强的结论是$\Pbb(x \in \Qbb \wedge [0,1]) = 0$

我的批注 用{==测度==} (可粗略理解为集合的大小) 的语言来说，单点集测度为零，有理数集的测度为零 (虽然它有可数无穷个元素)

<!-- slide data-notes="" -->

##### <span style="font-weight:900">1-</span>近邻序列的收敛性

---

引理：若$(\Xcal, \dist(\cdot, \cdot))$是{==可分==}度量空间，构造数据集序列

$$
\begin{align*}
    \quad D_1 = \{ \xv_1 \}, ~ D_2 = \{ \xv_1, \xv_2 \}, ~ \ldots, D_n = \{ \xv_1, \xv_2, \ldots, \xv_n \}, ~ \ldots, ~ \xv_i \overset{\text{iid}}{\sim} \Dcal
\end{align*}
$$

<div class="top-3"></div>

对$\forall \xv$，记$\xvhat_n = \mathop{\arg \min}_{\zv \in D_n} \dist (\xv, \zv)$，则 1-近邻序列$\xvhat_n \overset{\text{a.s.}}{\rightarrow} \xv$

证明：记$\xv$的邻域$B_\xv(r)$：以$\xv$为球心、$r$为半径的球

<div class="top-2"></div>

定义空间中的{==好点==}：对$\forall r > 0$有$\Pbb(B_\xv(r)) > 0$，于是

$$
\begin{align*}
    \quad \lim_{n \rightarrow \infty} \Pbb(\dist(\xvhat_n, \xv) > r) = \lim_{n \rightarrow \infty} (1 - \Pbb(B_\xv(r)))^n = 0
\end{align*}
$$

<div class="top-4"></div>

由$r$的任意性知$\lim_{n \rightarrow \infty} \Pbb(\dist(\xvhat_n, \xv) = 0) = 1$，从而$\xvhat_n \overset{\text{a.s.}}{\rightarrow} \xv$

已证“好点的 1-近邻序列收敛于自身的概率为 1”，如果“空间中好点的概率也为 1”，则结论成立

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">1-</span>近邻序列的收敛性

---

定义空间中的{==坏点==}：存在$r > 0$使得$\Pbb(B_\xv(r)) = 0$，设全部的坏点构成集合$N$，只需证$\Pbb(N) = 0$

<div class="width60">

根据{==可分性==}，$\Xcal$存在可数稠密子集$\Acal$，且存在点$\av \in B_\xv(r/3) \wedge \Acal$，考虑包含$\xv$的邻域$B_\av (r/2)$，易知其包含于$B_\xv(r)$，故$\Pbb(B_\av (r/2)) = 0$

</div>

<div class="width60">

每个坏点会对应一个以$\av$为球心的球，若多个坏点对应同一个$\av$，取并集，即半径最大的球，注意$\Acal$可数，因此最终只需可数个概率为零的球即可覆盖全部坏点，故$\Pbb(N) = 0$

</div>

@import "../tikz/knn-proof.svg" {.right4 .lefta .width36 .top-45per}

<!-- slide data-notes="" -->

##### 理论分析 <span style="font-weight:900">1-</span>近邻法

---

定理：设$(\Xcal, \dist(\cdot, \cdot))$是{==可分度量空间==}，标记集合$\Ycal = \{ 1, -1 \}$，贝叶斯最优分类器$h^\star(\xv) = \mathop{\arg\max}_{y \in \Ycal} \Pbb(y|\xv)$的错误率为$R^\star$，数据集$D = \{ (\xv_i, y_i) \}_{i \in [m]} \in (\Xcal \times \Ycal)^m$，1-近邻法的泛化错误率为$R_m$，其中下标$m$表示样本数，记$R = \lim_{m \rightarrow \infty} R_m$为渐进泛化错误率，则$R^\star \le R \le 2 R^\star (1 - R^\star)${==几乎必然==}成立

思路：在空间可分的条件下，1-近邻序列$\overset{\text{a.s.}}{\rightarrow} \xv$，从而 1-近邻的错误率 $\overset{\text{a.s.}}{\rightarrow}$ 在$\xv$处独立采样两次标记不同的概率

<div class="top1"></div>

$$
\begin{align*}
    \quad \Pbb(e|\xv) & \overset{\text{a.s.}}{\rightarrow} \Pbb(y=1|\xv)\Pbb(y=-1|\xv) + \Pbb(y=-1|\xv)\Pbb(y=1|\xv) \\
    & = 2 \Pbb(y=1|\xv) \Pbb(y=-1|\xv) \\
    & = 2 \underbrace{\Pbb(y=h^\star(\xv)|\xv)}_{h^\star(\xv)\text{的正确率}\qquad} \underbrace{(1 - \Pbb(y=h^\star(\xv)|\xv))}_{h^\star(\xv)\text{的错误率}\qquad} ~ \longleftarrow \text{接近上界的形式了}
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 理论分析 <span style="font-weight:900">1-</span>近邻法

---

证明：记$\Pbb (e^\star | \xv) = 1 - \Pbb(h^\star(\xv) | \xv)$，则$R^\star = \Ebb_\xv [\Pbb (e^\star | \xv)]$

对任意$\xv$，1-近邻法预测出错的概率为

$$
\begin{align*}
    \quad \Pbb(e|\xv) & = \Pbb(y=1|\xv) \Pbb(\yhat=-1|\xvhat) + \Pbb(y=-1|\xv) \Pbb(\yhat=1|\xvhat) \\
    & \overset{\text{a.s.}}{\rightarrow} 2 \Pbb(y=1|\xv) \Pbb(y=-1|\xv) = 2 \Pbb(e^\star|\xv) (1 - \Pbb(e^\star|\xv))
\end{align*}
$$

<div class="top-2"></div>

由于$\Pbb(e|\xv)$有界，根据勒贝格控制收敛定理

$$
\begin{align*}
    \quad \Pbb(e) & = \Ebb_\xv [\Pbb (e | \xv)] \\
    & \overset{\text{a.s.}}{\rightarrow} \Ebb_\xv [2 \Pbb(e^\star|\xv) (1 - \Pbb(e^\star|\xv))] \quad \longleftarrow \text{交换求积分和求极限} \\
    & = 2 \Ebb_\xv [\Pbb (e^\star | \xv)] - 2 \Ebb_\xv [\Pbb (e^\star | \xv)^2] \\
    & = 2 R^\star - 2 ({R^\star}^2 + \var [\Pbb (e^\star | \xv)]) \quad \longleftarrow \text{二阶矩 }=\text{ 期望 }^2 + \text{ 方差}  \\
    & \le 2 R^\star - 2 {R^\star}^2 \\
    & = 2 R^\star (1 - R^\star)
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 理论分析 推广到多类

---

设$\Ycal = [c]$，1-近邻的正确率 $\overset{\text{a.s.}}{\rightarrow}$ 在$\xv$处采样两次标记相同的概率

$$
\begin{align*}
    \quad \Pbb(e|\xv) = 1 - \sum_{j \in [c]} \Pbb(j|\xv)^2 = 1 - \Pbb(h^\star(\xv)|\xv)^2 - \sum_{j \neq h^\star(\xv)} \Pbb(j|\xv)^2
\end{align*}
$$

<div class="top-2"></div>

由柯西不等式

<div class="top-2"></div>

$$
\begin{align*}
    \quad \sum_{j \neq h^\star(\xv)} & \Pbb(j|\xv)^2 \geq \frac{( \sum_{j \neq h^\star(\xv)} \Pbb(j|\xv) )^2}{c-1}  = \frac{(1 - \Pbb(h^\star(\xv)|\xv))^2}{c-1} = \frac{\Pbb(e^\star|\xv)^2}{c-1}
\end{align*}
$$

<div class="top-2"></div>

回代有

<div class="top-2"></div>

$$
\begin{align*}
    \quad \Pbb(e|\xv) & \le 1 - (1 - \Pbb(e^\star|\xv))^2 - \frac{\Pbb(e^\star|\xv)^2}{c-1} = 2 \Pbb(e^\star|\xv) - \frac{c}{c-1} \Pbb(e^\star|\xv)^2 \\[4pt]
    \Longrightarrow R & \le 2 R^\star - \frac{c}{c-1} ({R^\star}^2 + \var [\Pbb (e^\star | \xv)]) \le R^\star \left( 2 - \frac{c}{c-1} R^\star \right)
\end{align*}
$$

<!-- slide data-notes="" -->

##### <span style="font-weight:900">k-</span>近邻 vs. <span style="font-weight:900">1-</span>近邻

---

当$m \rightarrow \infty$时有$R^\star \le \cdots \le R^{(5)} \le R^{(3)} \le R^{(1)} \le 2 R^\star (1 - R^\star)$

<div class="top-2"></div>

在$m$有限的情况下，$k$并不是越大越好，越大越欠拟合

考虑如下例子：$\Pbb(y=1) = \Pbb(y=-1) = 1/2$，$\Pbb(\xv | y=1)$均匀分布在$[99,101]$，$\Pbb(\xv | y=-1)$均匀分布在$[-101,-99]$

- 1-近邻出错：训练集中只有一类样本而待预测样本为另一类
- 3-近邻出错：训练集中某一类样本数$\le 1$，且待预测样本也为该类
- 5-近邻出错：训练集中某一类样本数$\le 2$，且待预测样本也为该类

<div class="top3"></div>

$$
\begin{align*}
    \quad & \Pbb(e|k=1) = \frac{1}{2} \frac{1}{2^m} + \frac{1}{2} \frac{1}{2^m}, \quad \Pbb(e|k=3) = \Pbb(e|k=1) + 2 \binom{m}{1} \frac{1}{2} \frac{1}{2^m} \\
    & \Pbb(e|k=5) = \Pbb(e|k=1) + \Pbb(e|k=3) + 2 \binom{m}{2} \frac{1}{2} \frac{1}{2^m}
\end{align*}
$$

<!-- slide data-notes="" -->

##### <span style="font-weight:900">k-</span>近邻法 变种

---

{==加权==}多数投票，例如权重取每个近邻到待预测样本距离的倒数

理论上加权没有优势，可以证明：最近邻权重取$1/k + \epsilon$、其它近邻权重取$1/k - \epsilon/(k-1)$时，泛化错误率大于标准的多数投票

<div class="top4"></div>

{==带拒绝的==}多数投票，例如$k$个近邻中至少$l$个近邻都属于同一类时才进行预测，设其错误率为$R^{(k,l)}$，理论上可以证明

$$
\begin{align*}
    \quad R^{(k,k)} \le R^{(k,k-1)} \le \cdots \le R^{(k,\lceil k/2 \rceil+1)} \le R^\star \le R^{(k,\lceil k/2 \rceil)} = R^{(k)}
\end{align*}
$$

我的批注 这里错误率低于贝叶斯最优分类器是因为计算错误率时只统计了预测的样本，难预测的样本都被拒绝了，没算在内

<!-- slide data-notes="" -->

##### 维度灾难 近邻不近

---

设$\Xcal = [0,1]^d$为$d$维单位立方体，训练样本在立方体内均匀分布

对任意待测试样本$\xv$，设包含其$k$-近邻的最小立方体的边长为$l$

$l^d \approx k / m$，则$l \approx \sqrt[d]{k/m}$，取$m=1000$、$k=10$

<div class="threelines column1-border1-right-solid-head row1-column1-border1-right-solid head-highlight-1 tr-hover row8-border-top-dashed">

| $d$ |  $2$  |   $3$   |  $10$   |  $100$  |  $1000$  |  $10000$  |
| :-: | :---: | :-----: | :-----: | :-----: | :------: | :-------: |
| $l$ | $0.1$ | $0.215$ | $0.631$ | $0.955$ | $0.9954$ | $0.99954$ |

</div>

当$d=1000$时，$10$-近邻近乎覆盖整个$\Xcal$，已经不是$\xv$的邻域了

<!-- slide vertical=true data-notes="" -->

##### 维度灾难 距离失效

---

在各维度下随机生成$1000$对样本，当维度很高时，任意一对样本的距离会集中在很小的范围内，没有比较的意义

@import "../python/dimension-curse.svg" {.center .width78 .top2}

<!-- slide data-notes="" -->

##### 维度灾难 超平面模型

---

$\av = [a_1; a_2; \ldots]$、$\bv = [b_1; b_2; \ldots]$，$\dist(\av,\bv) = \sqrt{\sum_j (a_j - b_j)^2}$

<div class="top-2"></div>

随着维度$d$增大，参与求和的分量越来越多，距离越来越大

引入超平面$w_1 x_1 + \cdots + w_d x_d + c = 0$

<div class="top-2"></div>

现假设维度$d \rightarrow d+1$，$a_{d+1}, w_{d+1} \overset{\text{iid}}{\sim} U[0,1]$，点$\av$到超平面距离

$$
\begin{align*}
    \quad \frac{|w_1 a_1 + \cdots + w_d a_d + c|}{\sqrt{w_1^2 + \cdots + w_d^2}} \longrightarrow \frac{|w_1 a_1 + \cdots + w_d a_d + \class{blue}{w_{d+1} a_{d+1}} + c|}{\sqrt{w_1^2 + \cdots + w_d^2 + \class{blue}{w_{d+1}^2}}}
\end{align*}
$$

<div class="top-4"></div>

点到超平面的距离能相对保持稳定，不受维度增加的影响

我的批注 高维空间中，感知机、对率回归、支持向量机等超平面模型往往效果很好

<!-- slide vertical=true data-notes="" -->

##### 维度灾难 超平面模型

---

高维空间中，与点和点之间的距离比，点到超平面的距离很小

当数据集有轻微扰动时，超平面模型也会发生变化，改变对部分样本的预测结果

这种扰动现在流行称为{==对抗样本==} (adversarial sample)，这种现象也通常归因于神经网络的模型复杂性，谬之大矣



