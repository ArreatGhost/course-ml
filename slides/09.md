---
presentation:
  margin: 0
  center: false
  transition: "convex"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## 贝叶斯概率

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide data-notes="" -->

##### 贝叶斯公式

---

- 华科有$80\%$科研水平高的老师
- 讲课水平和科研水平匹配的概率是$90\%$
- 张老师的机器学习课讲得有点次
- 张老师科研水平高的概率是多少？

两个随机变量：$\Theta$为张老师科研水平高，$X$为张老师讲课水平次

根据贝叶斯公式

$$
\begin{align*}
    \qquad \Pbb(\Theta|X) & = \frac{\Pbb(X|\Theta) \Pbb(\Theta)}{\Pbb(X)} = \frac{\Pbb(X|\Theta) \Pbb(\Theta)}{\Pbb(X|\Theta) \Pbb(\Theta) + \Pbb(X|\neg\Theta) \Pbb(\neg\Theta)} \\
    & = \frac{0.1 \times 0.8}{0.1 \times 0.8 + 0.9 \times 0.2} \approx 0.308
\end{align*}
$$

- 如果没上过张老师的课，只能根据经验盲猜概率是$80\%$
- 在观测到张老师讲课不好这一事实后，概率修正为$30.8\%$

<!-- slide vertical=true data-notes="" -->

##### 贝叶斯公式的新解释

---

$$
\begin{align*}
    \qquad \underbrace{\Pbb(\Theta|X)}_{\text{后验}} & = \frac{\overbrace{\Pbb(X|\Theta)}^{\text{似然}} \overbrace{\Pbb(\Theta)}^{\text{先验}}}{\underbrace{\Pbb(X)}_{\text{证据}}} = \Pbb(X|\Theta) \Pbb(\Theta) \Big/ \int \Pbb(X|\Theta) \Pbb(\Theta) \diff \Theta
\end{align*}
$$

- 先验 (_prior_) 对应模型的参数$\Theta$，例如科研水平伯努利分布的参数$80\%$
- 证据 (_evidence_) 对应观测到的数据$X$
- 似然 (_likelihood_) 是在给定参数$\Theta$的条件下观测到数据$X$的概率
- 后验 (_posterior_) 是在得到观测数据$X$后，对模型参数$\Theta$的修正

<div class="top2"></div>

{==在机器学习中只有观测数据$X$==}，此时估计参数$\Theta$的两种方法：

- 极大似然 (_<u>m</u>aximum <u>l</u>ikelihood_, ML)，$\Theta^{\text{ML}} = \mathop{\arg \max}_\Theta \Pbb(X|\Theta)$
- 最大后验 (_<u>m</u>aximum <u>a</u> <u>p</u>osterior_, MAP)，$\Theta^{\text{MAP}} = \mathop{\arg \max}_\Theta \Pbb(\Theta|X)$

<!-- slide data-notes="" -->

##### 频率主义

---

对{==不确定性==}的理解：

- {==概率==}是{==独立重复试验==}中不确定性事件 (随机事件) 发生{==频率==}的极限
- 观测数据是受参数空间里{==某个确定的未知参数==}控制生成的
- 欲知哪个值最有可能是这个参数，通过{==极大似然==}进行估计
- 估计有误差是因为数据有随机性，怎样刻画误差？置信区间
- 缺点：无法处理不可重复的事件？明天地球毁灭的概率是多少？

<div class="top2"></div>

以抛硬币为例

- 未知参数$\theta = \Pbb(\text{正面})$，与硬币相关，是确定的
- 观测数据$X$：$t$次抛掷中有$k$次正面，似然$\Pbb(X | \theta) = \binom{t}{k} \theta^k (1 - \theta)^{t-k}$
- 假设某次观测$X$是抛了$10$次全为正面，根据极大似然可知$\theta^{\text{ML}} = 1$
- 预测：第$11$次抛掷$100\%$是正面

<!-- slide data-notes="" -->

##### 贝叶斯主义

---

对{==不确定性==}的理解：

- 不确定性来自于{==观测者==}，与事件本身无关
- 地球毁灭之所以有不确定性，是因为{==观测者的信息不完备==}
- 概率是观测者对不确定性事件发生的{==信念==} (belief)

<div class="top2"></div>

以抛硬币为例

- $\theta = \Pbb(\text{正面})$不再是一个确定的数，而是$[0,1]$上的一个随机变量
- $\theta$的先验分布是观测者的初始信念
- $\theta$的后验分布是观测者接收到数据 (信息) 后对先验的修正

<div class="top2"></div>

优点：

- 通过先验可以很自然地引入{==领域知识==} (_domain knowledge_)
- 方便动态地处理数据，上一时刻的后验作为下一时刻的先验

<!-- slide vertical=true data-notes="" -->

##### 贝叶斯主义

---

数据$X$：$t$次抛掷中有$k$次正面，似然是{==二项式分布==}

$$
\begin{align*}
    \quad \Pbb(X | \theta) = \binom{t}{k} \theta^k (1 - \theta)^{t-k}
\end{align*}
$$

<div class="top-2"></div>

不妨设$\theta$的先验是参数为$(\alpha,\beta)$的{==贝塔分布==}：

$$
\begin{align*}
    \quad \Pbb(\theta) = \BetaDist(\theta|\alpha,\beta) = \frac{\theta^{\alpha - 1} (1-\theta)^{\beta - 1}}{\int_0^1 \theta^{\alpha - 1} (1-\theta)^{\beta - 1} \diff \theta} = \frac{\theta^{\alpha - 1} (1-\theta)^{\beta - 1}}{\BetaFunc(\alpha,\beta)}
\end{align*}
$$

<div class="top-4"></div>

其中$\BetaFunc(\alpha,\beta) = \int_0^1 \theta^{\alpha - 1} (1-\theta)^{\beta - 1} \diff \theta$是第一类欧拉积分

根据贝叶斯公式

$$
\begin{align*}
    \quad \Pbb(\theta|X) = \frac{\Pbb(\theta) \Pbb(X|\theta)}{\Pbb(X)} = \Pbb(\theta) \Pbb(X|\theta) \Big/ \int_0^1 \Pbb(\theta) \Pbb(X|\theta) \diff \theta
\end{align*}
$$

<!-- slide data-notes="" -->

##### 共轭先验

---

证据和后验分别为

$$
\begin{align*}
    \quad \Pbb(X) & = \int_0^1 \Pbb(\theta) \Pbb(X|\theta) \diff \theta = \binom{t}{k} \frac{1}{\BetaFunc(\alpha,\beta)} \int_0^1  \theta^{\alpha + k - 1} (1 - \theta)^{\beta + t-k-1} \diff \theta \\
    & = \binom{t}{k} \frac{\BetaFunc(\alpha+k,\beta+t-k)}{\BetaFunc(\alpha,\beta)} \\[10pt]
    \Pbb(\theta|X) & = \frac{\Pbb(\theta) \Pbb(X|\theta)}{\Pbb(X)} = \frac{\theta^{\alpha + k - 1} (1-\theta)^{\beta + t - k - 1}}{\BetaFunc(\alpha+k,\beta+t-k)} = \BetaDist(\theta|\alpha+k,\beta+t-k)
\end{align*}
$$

- 似然$\Pbb(X | \theta)$是二项式分布，先验也凑$\theta^\spadesuit (1-\theta)^\heartsuit$的形式，选贝塔分布
- 与似然相乘后，后验与先验属同一分布族，仅参数有变化
- 贝塔分布是二项式分布的{==共轭先验==} (_conjugate prior_)
- 先验分布的参数$(\alpha,\beta)$既可视为观测者的领域知识，也可视为“伪数据”：在观测到$X$前还观测过$\alpha+\beta$次抛掷中有$\alpha$次正面

<!-- slide vertical=true data-notes="" -->

##### 最大后验 全贝叶斯

---

后验为

$$
\begin{align*}
    \qquad \Pbb(\theta|X) = \frac{\theta^{\alpha + k - 1} (1-\theta)^{\beta + t - k - 1}}{\BetaFunc(\alpha+k,\beta+t-k)} = \BetaDist(\theta|\alpha+k,\beta+t-k)
\end{align*}
$$

<div class="top-2"></div>

若目标就是估计$\theta$，则采用 MAP 估计：$\theta^{\text{MAP}} = \mathrm{\arg \max}_{\theta} ~ \Pbb(\theta|X)$

若目标是对未知样本$\xhat$做预测，有两种做法：

- 先估计$\theta^{\text{MAP}}$再计算$\Pbb(\xhat|\theta^{\text{MAP}})$，但这样做忽略了$\theta$本身的随机性，尤其当后验$\Pbb(\theta|X)$是个{==平坦==}的分布时，只取一个点来做决策风险很大
- {==全贝叶斯==} (fully Bayesian) 预测：考虑所有的$\theta$，根据后验加权

<div class="top2"></div>

$$
\begin{align*}
    \qquad \Pbb(\xhat|X) = \int \Pbb(\xhat|\theta) \Pbb(\theta|X) \diff \theta
\end{align*}
$$

<!-- slide data-notes="" -->

##### 频率 _vs._ 贝叶斯

---

在机器学习中体现出的区别：是否考虑先验

当观测数据量很大时，先验 (伪数据) 就无足轻重了，两种做法不会有太大差别

当观测数据量不大时，先验对模型性能有显著影响 (归纳偏好)

- 先验是主观的，纯人为选取，没有标准
- 抛硬币问题选贝塔分布做先验就是图计算方便
- 利用共轭先验可以不用积分显式地求$\Pbb(X)$，肉眼就能看出结果

<div class="top2"></div>

先验需有适当的自由度，通过调参数灵活表示领域知识
