---
presentation:
  margin: 0
  center: false
  transition: "convex"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## 基本概念 下

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide vertical=true data-notes="" -->

##### 机器学习

---

一般流程

@import "../dot/ml-old.dot" {.top0 .bottom0}

<div class="bottom0"></div>

原始数据：表格、图片、视频、文本、语音、……

<div class="bottom-2"></div>

特征工程：

- 提取：选取、构造对目标任务有用的潜在特征
- 处理：无序的离散类别特征 → 数值特征，缺失处理，标准化
- 变换：对特征进行挑选或映射得到对目标任务更有效的特征

模型学习：最核心的部分，学习一个用来预测的映射

<!-- slide data-notes="" -->

##### 特征提取 以文本为例

---

<span class="blue">词袋模型</span> (bag-of-words)：文本是单词的集合，单词独立无序

- 所有文本全部$d$个不同的单词构成词典，每个文本提取$d$个特征
- 若词典第$i$个词在当前文本中出现过，则其第$i$个特征为$1$，否则为$0$

```python {.line-numbers .top2 .left4 highlight=[4-5]}
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

document1 = "I have a pen, I have an apple, apple pen."
document2 = "I have a pen, I have pineapple, pineapple pen."

cv = CountVectorizer(lowercase=False, token_pattern='\w+', binary=True)
model = cv.fit_transform([document1, document2])
pd.DataFrame(model.toarray(), columns=cv.get_feature_names_out())
```

<div class="threelines column1-border-right-solid column1-bold head-highlight-1 tr-hover top-4">

|  词典  |  I  |  a  | an  | apple | have | pen | pineapple |
| :----: | :-: | :-: | :-: | :---: | :--: | :-: | :-------: |
| 文本 1 |  1  |  1  |  1  |   1   |  1   |  1  |     0     |
| 文本 2 |  1  |  1  |  0  |   0   |  1   |  1  |     1     |

</div>

<!-- slide vertical=true data-notes="" -->

##### 特征提取 以文本为例

---

<span class="blue">词袋模型</span> (bag-of-words)：文本是单词的集合，单词独立无序

- 所有文本全部$d$个不同的单词构成词典，每个文本提取$d$个特征
- 若词典第$i$个词在当前文本中出现了$k$次，则其第$i$个特征为$k$

```python {.line-numbers .top2 .left4 highlight=[4-5]}
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

document1 = "I have a pen, I have an apple, apple pen."
document2 = "I have a pen, I have pineapple, pineapple pen."

cv = CountVectorizer(lowercase=False, token_pattern='\w+')
model = cv.fit_transform([document1, document2])
pd.DataFrame(model.toarray(), columns=cv.get_feature_names_out())
```

<div class="threelines column1-border-right-solid column1-bold head-highlight-1 tr-hover top-4">

|  词典  |  I  |  a  | an  | apple | have | pen | pineapple |
| :----: | :-: | :-: | :-: | :---: | :--: | :-: | :-------: |
| 文本 1 |  2  |  1  |  1  |   2   |  2   |  2  |     0     |
| 文本 2 |  2  |  1  |  0  |   0   |  2   |  2  |     2     |

</div>

<!-- slide vertical=true data-notes="" -->

##### 特征提取 以文本为例

---

<span class="blue">词频 - 逆文本频率</span>特征：对当前文本重要的单词必然

- 在当前文本中出现的频率高，即词频 (term frequency, tf) 高
- 在其他文本中出现的频率低，即逆文本频率 (inverse document frequency, idf) 高

tf = 单词在当前文本中出现的次数 / 当前文本的总词数<br>idf = ln ((全部文本数 + C) / (包含该词的总文本数 + C)) + 1</span>

- C = 0，若词典包含从未在任何文本中出现的词，会有分母为零的问题
- C = 1，sklearn 默认的平滑版本，相当于额外有一个包含所有词的文本

tf - idf 特征 = normalize (tf × idf)，即将 tf 和 idf 相乘后再标准化

- $\ell_1$标准化，tf × idf / sum (tf × idf)，即线性变换成概率分布
- $\ell_2$标准化，tf × idf / sqrt(sum ([tf × idf]^2))，即线性变换成模 1 向量

<!-- slide vertical=true data-notes="" -->

##### 特征提取 以文本为例

---

```python {.line-numbers .top-1 .left4 highlight=[4-5]}
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

document1 = "I have a pen, I have an apple, apple pen."
document2 = "I have a pen, I have pineapple, pineapple pen."

tv = TfidfVectorizer(lowercase=False, token_pattern='\w+',
                     norm='l1', smooth_idf=False) # l1归一化 idf不平滑
model = tv.fit_transform([document1, document2])
pd.DataFrame(model.toarray(), columns=cv.get_feature_names_out())
```

<div class="threelines row3-border-top-dashed row3-border-bottom-dashed column1-border1-right-dashed-head row1-column1-border1-right-dashed row3-column1-border1-right-dashed row4-column1-border1-right-dashed head-highlight-1 tr-hover top-4">

|   词典   |     I     |     a     |    an     |   apple   |   have    |    pen    | pineapple |
| :------: | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: |
|    tf    |  2 / 10   |  1 / 10   |  1 / 10   |  2 / 10   |  2 / 10   |  2 / 10   |     0     |
|    ^     |   2 / 9   |   1 / 9   |     0     |     0     |   2 / 9   |   2 / 9   |   2 / 9   |
|   idf    | ln(1) + 1 | ln(1) + 1 | ln(2) + 1 | ln(2) + 1 | ln(1) + 1 | ln(1) + 1 | ln(2) + 1 |
| tf - idf |   0.165   |   0.082   |   0.140   |   0.280   |   0.165   |   0.165   |   0.000   |
|    ^     |   0.192   |   0.096   |   0.000   |   0.000   |   0.192   |   0.192   |   0.326   |

</div>

<!-- slide data-notes="" -->

##### 特征 离散类别 → 数值

---

<div class="threelines column7-border-right-solid head-highlight-1 tr-hover">

| 次序 | 时间 | 方式 | 天气 | 课业 | 疫情 | 电视 | 约会 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  1   | 周六 | 吃饭 | 晴天 | 轻松 | 清零 | 精彩 |  是  |
|  6   | 周六 | 逛街 | 晴天 | 轻松 | 平缓 | 无聊 |  是  |
|  10  | 周六 | 学习 | 雨天 | 轻松 | 严峻 | 无聊 |  否  |
|  13  | 周六 | 逛街 | 晴天 | 正常 | 清零 | 精彩 |  否  |

</div>

- {==序数编码==} (ordinal encoding)：清零 - 0、平缓 - 1、严峻 - 2，需类别特征本身有序，否则若吃饭 - 0、逛街 - 1、学习 - 2，为何 |吃饭 - 学习| > |吃饭 - 逛街| ？
- {==独热编码==} (one-hot encoding)：吃饭 - 001、逛街 - 010、学习 - 100，一碗水端平，所有取值距离相等，但若取值很多码会很长，且不适应动态出现的新取值
- {==哈希编码==} (hash encoding)：用哈希函数将任意输入映射到有限整数范围，码长固定，也能适应动态出现的新取值，但可能存在信息丢失

<!-- slide vertical=true data-notes="" -->

##### 特征独热编码

---

```python {.line-numbers .top-1 .left4 highlight=[13,17-20,23-24]}
import numpy as np
from sklearn.preprocessing import LabelBinarizer, OneHotEncoder

X = np.array([
    [ 1, '周六', '吃饭', '晴天', '轻松', '清零', '精彩'],
    [ 6, '周六', '逛街', '晴天', '轻松', '平缓', '无聊'],
    [10, '周六', '学习', '雨天', '轻松', '严峻', '无聊'],
    [13, '周六', '逛街', '晴天', '正常', '清零', '精彩'],
])
y = np.array(['是', '是', '否', '否'])

LabelBinarizer().fit_transform(y).squeeze()  # 标记二值化
[1 1 0 0]

enc = OneHotEncoder()
enc.fit_transform(X[:,1:7]).toarray() # 对6个类别特征采用独热编码
[[1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.],
 [1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.],
 [1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.],
 [1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.]]

enc.get_feature_names_out() # 独热编码对应的原始特征
['x0_周六', 'x1_吃饭', 'x1_学习', 'x1_逛街', 'x2_晴天', 'x2_雨天', 'x3_正常',
 'x3_轻松', 'x4_严峻', 'x4_平缓', 'x4_清零', 'x5_无聊', 'x5_精彩']
```

<!-- slide data-notes="" -->

##### 特征缺失处理

---

<div class="threelines column9-border-right-solid head-highlight-1 tr-hover">

| 次序 | 时间 | 方式 | 天气 | 课业 | 疫情 | 电视 | 温度 | 距离 | 约会 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  1   | 周六 | 吃饭 | 晴天 | 轻松 | 清零 | 精彩 | 25.2 | 0.5  |  是  |
|  6   | 周六 | 逛街 | 晴天 | 轻松 | 平缓 | 无聊 |  -   | 2.0  |  是  |
|  10  | 周六 |  -   | 雨天 | 轻松 | 严峻 | 无聊 | 32.6 | 8.2  |  否  |
|  13  | 周六 | 逛街 | 晴天 | 正常 | 清零 | 精彩 | 36.4 | 9.8  |  否  |

</div>

删除：直接删除有特征缺失的样本，简单粗暴，信息损失

补全：

- 用未缺失该特征的样本计算{==平均数==}、{==中位数==}、{==众数==}填充，引入噪声？
- 用没有缺失的特征{==学习并预测==}缺失特征的取值，若两者之间无关？
- 将“缺失”本身作为一种特征取值

<!-- slide vertical=true data-notes="" -->

##### 特征缺失处理

---

```python {.line-numbers .top-1 .left4 highlight=[14-17,21-24]}
import numpy as np
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import SimpleImputer, IterativeImputer

X = np.array([
    [1, '周六', '吃饭', '晴天', '轻松', '清零', '精彩', 25.2, 0.5],
    [6, '周六', '逛街', '晴天', '轻松', '平缓', '无聊', np.nan, 2.0],
    [10, '周六', '-', '雨天', '轻松', '严峻', '无聊', 32.6, 8.2],
    [13, '周六', '逛街', '晴天', '正常', '清零', '精彩', 36.4, 9.8],
])

imp_mean = SimpleImputer(strategy='mean')
imp_mean.fit_transform(X[:,[7]])  # 用均值填充
[[25.2],
 [31.4],
 [32.6],
 [36.4]]

imp_median = SimpleImputer(strategy='median')
imp_median.fit_transform(X[:,[7]]) # 用中位数填充
[[25.2],
 [32.6],
 [32.6],
 [36.4]]
```

<!-- slide vertical=true data-notes="" -->

##### 特征缺失处理

---

```python {.line-numbers .top-1 .left4 highlight=[10-13,21-24]}
X = np.array([
    [1, '周六', '吃饭', '晴天', '轻松', '清零', '精彩', 25.2, 0.5],
    [6, '周六', '逛街', '晴天', '轻松', '平缓', '无聊', np.nan, 2.0],
    [10, '周六', '-', '雨天', '轻松', '严峻', '无聊', 32.6, 8.2],
    [13, '周六', '逛街', '晴天', '正常', '清零', '精彩', 36.4, 9.8],
])

imp_frequent = SimpleImputer(missing_values='-', strategy='most_frequent')
imp_frequent.fit_transform(X[:,[2]].astype('object')) # 用众数填充
[['吃饭'],
 ['逛街'],
 ['逛街'],
 ['逛街']]

# 回归器默认采用BayesianRidge
# 也可选DecisionTreeRegressor ExtraTreesRegressor KNeighborsRegressor

from sklearn.neighbors import KNeighborsRegressor
imp_iter = IterativeImputer(estimator=KNeighborsRegressor(n_neighbors=2))
imp_iter.fit_transform(X[:,[0,7,8]])
[[ 1. , 25.2,  0.5],
 [ 6. , 28.9,  2. ],
 [10. , 32.6,  8.2],
 [13. , 36.4,  9.8]]
```

<!-- slide data-notes="" -->

##### 特征标准化

---

也称归一化，旨在<span class="blue">消除不同特征间的量纲影响</span>

离差标准化：将原始特征线性变换到 [0, 1] 区间

$$
\begin{align*}
    \qquad x \leftarrow \frac{x - x_\min}{x_\max - x_\min} \in [0,1]
\end{align*}
$$

最大值标准化：除以该特征的绝对值最大值

$$
\begin{align*}
    \qquad x \leftarrow \frac{x}{\max_{i \in [m]} |x_i|} \in [-1,1]
\end{align*}
$$

标准差标准化：经过处理的特征近似符合标准正态分布$\Ncal(0,1)$

$$
\begin{align*}
    \qquad x \leftarrow \frac{x - \mu}{\sigma}, \quad x \leftarrow \frac{x - x_{\text{median}}}{\sum_{i \in [m]} |x_i - x_{\text{median}}| / m}
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 离差与最大值标准化

---

```python {.line-numbers .top-1 .left4 highlight=[12-15,18-21]}
import numpy as np
from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler

X = np.array([
    [1, '周六', '吃饭', '晴天', '轻松', '清零', '精彩', 25.2, 0.5],
    [6, '周六', '逛街', '晴天', '轻松', '平缓', '无聊', 27.4, 2.0],
    [10, '周六', '学习', '雨天', '轻松', '严峻', '无聊', 32.6, 8.2],
    [13, '周六', '逛街', '晴天', '正常', '清零', '精彩', 36.4, 9.8],
])

MinMaxScaler().fit_transform(X[:,[0,7,8]]) # 最大值变成1 同时 最小值变成0
[[0.        , 0.        ],
 [0.33333333, 0.27272727],
 [0.66666667, 0.36363636],
 [1.        , 1.        ]]

MaxAbsScaler().fit_transform(X[:,[0,7,8]]) # 最大值变成1
[[0.07692308, 0.69230769, 0.05102041],
 [0.46153846, 0.75274725, 0.20408163],
 [0.76923077, 0.8956044 , 0.83673469],
 [1.        , 1.        , 1.        ]]
```

<!-- slide vertical=true data-notes="" -->

##### 标准差标准化

---

```python {.line-numbers .top-1 .left4 highlight=[13-16,19,22]}
import numpy as np
from sklearn.preprocessing import scale

X = np.array([
    [1, '周六', '吃饭', '晴天', '轻松', '清零', '精彩', 25.2, 0.5],
    [6, '周六', '逛街', '晴天', '轻松', '平缓', '无聊', 27.4, 2.0],
    [10, '周六', '学习', '雨天', '轻松', '严峻', '无聊', 32.6, 8.2],
    [13, '周六', '逛街', '晴天', '正常', '清零', '精彩', 36.4, 9.8],
])

x = scale(X[:,[0,7,8]])
x
[[-1.44444444, -1.1861146 , -1.17034706],
 [-0.33333333, -0.68429689, -0.79077504],
 [ 0.55555556,  0.50181772,  0.77812264],
 [ 1.22222222,  1.36859377,  1.18299946]]

x.mean(axis=0) # 均值为0
[ 5.55111512e-17,  2.22044605e-16, -5.55111512e-17]

x.std(axis=0) # 标准差为1
[1., 1., 1.]
```
