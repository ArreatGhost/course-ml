---
presentation:
  margin: 0
  center: false
  transition: "convex"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"

<!-- slide data-notes="" -->

##### 课程群

---

<img src="../img/qr.png" width=414px height=418px class="center top10">

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## 绪论

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide vertical=true data-notes="" -->

##### 课程安排

---

授课：张腾

时间：40 学时

- 28 学时理论，周三 7 ~ 8 节课、周五 1 ~ 2 节课 (第 10 ~ 16 周)
- 12 学时实验，周四 9 ~ 12 节课 (第 12、15 ~ 16 周，在线)

考核：1 ~ 2 次大作业 (细节待定)

<!-- slide vertical=true data-notes="" -->

##### 参考书目

---

<div class="top-2">
    <img src="../img/book/ml.png" width=200px height=270px>
    <img src="../img/book/tml.png" width=200px height=270px>
    <img src="../img/book/sml.png" width=200px height=270px>
</div>

<div class="top-5">
    <img src="../img/book/foml.png" width=200px height=270px>
    <img src="../img/book/uml.png" width=200px height=270px>
    <img src="../img/book/prml.png" width=200px height=270px>
</div>

<!-- slide data-notes="" -->

##### 引言

---

@import "../puml/ml.puml" {.center}

<!-- slide data-background-video="../videos/facial-recognition.webm" data-background-video-loop data-background-video-muted vertical=true data-notes="" -->

<!-- slide data-background-video="../videos/alphago.mp4" data-background-video-loop data-background-video-muted vertical=true data-notes="可能有人会想到alphago，这个围棋程序16年一经面世就横扫围棋界，将李世乭、柯杰等多位世界冠军杀得毫无还手之力" -->

<!-- slide data-background-video="../videos/self-driving.mp4" data-background-video-loop data-background-video-muted vertical=true data-notes="可能也有人会想到自动驾驶，现在百度、谷歌、特斯拉、比亚迪许多公司都在布局研究，我们国家也在出政策大力扶持" -->

<!-- slide data-notes="" -->

##### 引言

---

机器学习接管生活

<div>
    <img src="../img/app/recommendation.jpg" width=170px>
    <img src="../img/app/spam.jpg" width=170px>
    <img src="../img/app/translation.jpg" width=170px>
    <img src="../img/app/stock.jpg" width=170px>
    <img src="../img/app/navigation.jpg" width=170px>
</div>

<!-- slide data-transition="convex-in none" vertical=true data-notes="前三个对应前面的视频" -->

##### 引言

---

@import "../dot/ml-app.dot" {.left12}

<!-- slide data-transition="none convex-out" vertical=true data-notes="前三个对应前面的视频" -->

##### 引言

---

@import "../dot/ml-app-dev.dot" {.left12}

<!-- slide data-notes="" -->

##### 与传统算法区别

---

@import "../dot/csalg-mlalg.dot" {.center}

### <span class="fragment">机器学习算法是一种<span class="yellow">元算法</span> (_meta algorithm_)</span>

<!-- slide vertical=true data-notes="" -->

##### 举个例子

---

<img src="../img/long.jpg" class="width30 left10">

<div class="width60 top-60per left38per">

> 寻龙诀：<br>寻龙分金看缠山，一重缠是一重关，关门如有八重险，不出阴阳八卦形。发丘印，摸金符，搬山卸岭寻龙诀；人点蜡，鬼吹灯，勘舆倒斗觅星峰；水银癍，养明器，龙楼宝殿去无数；窨沉棺，青铜椁，八字不硬莫近前。入口为马，马为离卦；子鼠遇马为坎离；未羊遇马为坤离；戌狗遇马为乾离，上乾下离见生门。

</div>

<!-- slide vertical=true data-notes="" -->

##### 举个例子

---

<img src="../img/shan.jpg" class="center bottom4">

- 传统算法 (寻龙诀)：山川地势 → 有无大墓
- 机器学习算法：(山川地势, 有无大墓) 的数据 → 寻龙诀

<div class="bottom4"></div>

#### <span class="fragment">机器学习算法早已用于分析遥感图像，检测石油矿产等资源</span>

<!-- slide vertical=true data-notes="" -->

##### 多学科交叉

---

<div id="venn-ml" class="center"></div>

<!-- slide data-notes="" -->

##### 动手实践

---

编程语言：Python (首选)、Julia、Matlab

机器学习必备的第三方开源扩展包

- NumPy：针对高维矩阵、数组运算的数学函数库
- pandas：用于操纵数值表格和时间序列的函数库
- SciPy：用于最优化、线性代数、积分、插值的数学函数库
- Matplotlib：绘图库
- scikit-learn：机器学习库
- TensorFlow：谷歌开发的神经网络函数库
- PyTorch：脸书开发的神经网络函数库

<div class="bottom2"></div>

第三方扩展包之间依赖关系复杂，新手推荐用 Anaconda

开发环境个人偏爱 VS Code

<!-- slide data-background-iframe="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/" vertical=true data-background-interactive data-notes="" -->

<!-- slide data-notes="" -->

##### 大纲

---

@import "../vega/outline.json" {as="vega" .top-2}

<!-- slide vertical=true data-notes="" -->

##### 背景

---

维纳 Norbert Wiener 1894.11.26 ~ 1964.3.18 《控制论》

> 第一次工业革命：用某种机器来减轻甚至代替<span class="blue">体力</span>劳动<br>
> 上世纪中叶：用某种新型机器来减轻甚至代替<span class="blue">脑力</span>劳动

关键：让机器具有人类的智能

问题：什么是智能？

<!-- slide data-notes="" -->

##### 起源

---

图灵 Alan Turing 1912.6.23 ~ 1954.6.7

1950《Computing Machinery and Intelligence》

<div>
    <img src="../img/turing/1.jpg" width=271px height=369px>
    <img src="../img/turing/2.jpg" width=271px height=369px>
    <img src="../img/turing/3.gif" width=271px height=369px>
</div>

<!-- slide vertical=true data-notes="" -->

##### 图灵测试

---

1950《Computing Machinery and Intelligence》

<span class="blue">图灵测试</span>：一个人在不接触对方的情况下，通过一种特殊的方式，和对方进行一系列的问答，如果在相当长时间内，他无法根据这些问题判断对方是人还是计算机，那么就可以认为这个计算机是智能的

</div>

要想通过图灵测试，机器得具备多种能力

- 学习：机器学习
- 感知：计算机视觉，语音识别
- 认知：自然语言处理，知识表示

<!-- slide data-notes="" -->

##### 元年 达特茅斯会议

---

1956 达特茅斯会议 十仙过海

<div class="top-2">
    <img src="../img/meeting/John_McCarthy_Stanford.png" title="麦卡锡 1927 ~ 2011" width=171px height=228px>
    <img src="../img/meeting/Marvin_Minsky_at_OLPCb.png" title="明斯基 1927 ~ 2016" width=171px height=228px>
    <img src="../img/meeting/ClaudeShannon_MFO3807.png" title="香农 1916 ~ 2001" width=171px height=228px>
    <img src="../img/meeting/simon.png" title="西蒙 1916 ~ 2001" width=171px height=228px>
    <img src="../img/meeting/allen_newell.png" title="纽厄尔 1927 ~ 1992" width=171px height=228px>
</div>

<div class="top-5">
    <img src="../img/meeting/Oliver_Selfridge_at_BBN.png" title="赛弗里奇" width=171px height=228px>
    <img src="../img/meeting/Arthur_Samuel.png" title="塞缪尔" width=171px height=228px>
    <img src="../img/meeting/N. rochester.png" title="伯恩斯坦" width=171px height=228px>
    <img src="../img/meeting/Trenchard_More.png" title="摩尔" width=171px height=228px>
    <img src="../img/meeting/Ray_Solomonoff.png" title="所罗门诺夫 1926 ~ 2009" width=171px height=228px>
</div>

<!-- slide vertical=true data-notes="" -->

##### 人物关系

---

@import "../dot/dartmouth.dot" {.center}

<!-- slide vertical=true data-notes="" -->

##### 会议内容

---

七个研究方向：

- 自动计算机，“自动”指的是可编程
- 编程语言
- 神经网络
- 计算规模理论，即计算复杂性
- 自我改进，即机器学习
- 抽象
- 随机性和创见性

<div class="top4"></div>

干货：西蒙、纽厄尔的“逻辑理论家” (Logic Theorist)

- 第一个可运行的人工智能程序
- 证明罗素《数学原理》中的命题逻辑定理

<!-- slide vertical=true data-notes="" -->

##### 忆往昔峥嵘岁月稠

---

<div class="multi_column top_2">
    <img src="../img/birth/he1.jpg" title="当年的合影" width=425px height=277px class="left6">
    <img src="../img/birth/birth-people.jpg" title="2006年会议50周年时还健在的5位参会者" width=425px height=277px class="right6 lefta">
</div>

有些人会后就离开科研一线，去工业界赚钱去了

有些人后来出坑回踩，走向了人工智能的对立面

<!-- slide data-notes="" -->

##### 发展历史

---

@import "../mermaid/ai.mermaid"

<!-- slide vertical=true data-notes="" -->

##### 推理期

---

机器擅长固定套路的计算 vs. 人类擅长妙手偶得的推理

西蒙、纽厄尔设计了“逻辑理论家”程序

- 1952 年证明了《数学原理》中的 38 条定理
- 1963 年证明了《数学原理》中的 52 条定理，定理 2.85 的证明更巧妙

<div class="bottom2"></div>

王浩、吴文俊的工作：

- 1959 年，王浩证明了《数学原理》中的 150 条一阶逻辑定理和 200 条命题逻辑定理，关于复杂性的研究引出学生库克的 NP 理论
- 1978 年，吴文俊给出几何定理证明

<div class="bottom2"></div>

衰退：并非所有问题都可以转换成推理问题

- 十万步内无法证明<span class="blue">两个连续函数之和还是连续函数</span>

<!-- slide vertical=true data-notes="" -->

##### 谁说了实话

---

- A：B 和 C 都是说谎者
- B：A 和 C 都是说谎者
- C：A 和 B 中至少有一个说谎者

<div class="bottom1"></div>

根据 A 说实话与否有 (利用$p \rightarrow q \Longleftrightarrow \neg p \vee q$)

- $A \rightarrow \neg B \wedge \neg C \Longleftrightarrow A \rightarrow \neg B, ~ A \rightarrow \neg C \Longleftrightarrow (1)~\neg A \vee \neg B, ~ (2)~ \neg A \vee \neg C$
- $\neg A \rightarrow B \vee C \Longleftrightarrow (3)~ A \vee B \vee C$

<div class="bottom1"></div>

根据 B 说实话与否有

- $B \rightarrow \neg A \wedge \neg C \Longleftrightarrow B \rightarrow \neg A, ~ B \rightarrow \neg C \Longleftrightarrow (4)~\neg B \vee \neg C$

<div class="bottom1"></div>

根据 C 说实话与否有

- $C \rightarrow \neg A \vee \neg B \Longleftrightarrow (5)~\neg A \vee \neg B \vee \neg C$
- $\neg C \rightarrow A \wedge B \Longleftrightarrow (6)~A \vee C, ~ (7)~B \vee C $

<!-- slide vertical=true data-notes="" -->

##### 归结原理

---

1. $\neg A \vee \neg B$
2. $\neg A \vee \neg C$
3. $A \vee B \vee C$
4. $\neg B \vee \neg C$
5. $\neg A \vee \neg B \vee \neg C$
6. $A \vee C$
7. $B \vee C$

<div class="bottom1"></div>

利用归结原理 $P \vee Q, ~ \neg P \vee R \Longrightarrow Q \vee R$ 消除变量

8. 1 + 7：$\neg A \vee C$
9. 6 + 8：$C$，C 是老实人
10. 2 + 8：$\neg A$，A 是说谎者
11. 4 + 9：$\neg B$，B 是说谎者

<!-- slide data-notes="" -->

##### 知识期

---

教训：光有逻辑推理远远不够，机器得拥有知识

<div class="bottom2"></div>

信仰：<span class="blue">知识就是力量</span>

<div class="bottom2"></div>

专家系统 = 知识库 + 推理机

- 在特定领域内具有专家水平解决问题能力的程序系统
- 第一个成功的专家系统 DENDRAL 于 1965 年问世
- 知识工程之父费根鲍姆获得了 1994 年的图灵奖

<div class="bottom2"></div>

衰退：

- 人工构建知识库成本太高
- 很多知识获取困难，甚至无法被清晰地表示出来

<!-- slide vertical=true data-notes="" -->

##### 动物识别专家系统

---

@import "../dot/reasoning.dot"

<!-- slide data-notes="对于人类的很多智能行为，我们很难描述出其背后的知识" -->

##### 学习期

---

我们人类是如何识别下面的手写数字的？

@import "../img/number.svg" {.center .width64}

<!-- slide vertical=true data-notes="" -->

##### 学习期

---

我们人类是如何判断下面这首词的情感的？

> 明月几时有？把酒问青天。不知天上宫阙，今夕是何年。我欲乘风归去，又恐琼楼玉宇，高处不胜寒。起舞弄清影，何似在人间。<br>
> 转朱阁，低绮户，照无眠。不应有恨，何事长向别时圆？人有悲欢离合，月有阴晴圆缺，此事古难全。但愿人长久，千里共婵娟。

<!-- slide vertical=true data-notes="" -->

##### 学习期

---

我们人类是如何识别下面的语音的？

<svg version="1.1" xmlns="http://www.w3.org/2000/svg" style="display:none;color:#586e75;"><symbol id="pause" viewBox="0 0 1024 1024"><path d="M512 1024C229.696 1024 0 794.304 0 512S229.696 0 512 0s512 229.696 512 512-229.696 512-512 512zm0-960C264.96 64 64 264.96 64 512s200.96 448 448 448 448-200.96 448-448S759.04 64 512 64z"/><path d="M416 352h64v320h-64V352zm128 0h64v320h-64V352z"/></symbol><symbol id="play" viewBox="0 0 1024 1024"><path d="M512 0C229.228 0 0 229.241 0 512s229.228 512 512 512c282.759 0 512-229.241 512-512S794.759 0 512 0zm0 970.105c-253.009 0-458.105-205.11-458.105-458.105S258.99 53.895 512 53.895c252.995 0 458.105 205.11 458.105 458.105S764.995 970.105 512 970.105zM377.263 727.58l377.263-215.552L377.263 296.42V727.58z"/></symbol></svg>

<div id="waveform" class="waveform"></div>
<p style="text-align:center"><a href="javascript:" id="btnPlay" role="button" style="margin:0;"><svg><use href="#play"/></svg></a>
<a href="javascript:" id="btnPause" role="button" style="margin:0;"><svg><use href="#pause"/></svg></a></p>

@import "../js/wavesurfer/wavesurfer.js"
@import "../js/wavesurfer/play-audio.js"

<h3 class="fragment top10">让<span class="blue">机器</span>从数据中自动<span class="blue">学习</span>得到某种知识</h3>

<!-- slide vertical=true data-notes="" -->

##### 学习期

---

基本想法：让<span class="blue">机器</span>从数据中自动<span class="blue">学习</span>得到某种知识 (规律)

@import "../dot/ml-old.dot"

<div class="bottom0"></div>

原始数据：表格、图片、视频、文本、语音、……

特征工程：

- 提取：选取对目标任务有用的潜在特征
- 处理：无序的离散类别特征 → 数值特征，缺失处理，标准化
- 变换：对特征进行挑选或映射得到对目标任务更有效的特征

模型学习：最核心的部分，学习一个特征到类别标记的映射

<!-- slide data-notes="" -->

##### 特征提取 以文本为例

---

<span class="blue">词袋模型</span> (bag-of-words)：文本是单词的集合，单词独立无序

- 所有文本全部$d$个不同的单词构成词典，每个文本提取$d$个特征
- 若词典第$i$个词在当前文本中出现过，则其第$i$个特征为$1$，否则为$0$

```python {.line-numbers .top0 .left4 highlight=[4-5]}
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

document1 = "I have a pen, I have an apple, apple pen."
document2 = "I have a pen, I have pineapple, pineapple pen."

cv = CountVectorizer(lowercase=False, token_pattern='\w+', binary=True)
model = cv.fit_transform([document1, document2])
pd.DataFrame(model.toarray(), columns=cv.get_feature_names_out())
```

<div class="threelines column1-border-right-solid column1-bold head-highlight-1 tr-hover top-4">

|  词典  |  I  |  a  | an  | apple | have | pen | pineapple |
| :----: | :-: | :-: | :-: | :---: | :--: | :-: | :-------: |
| 文本 1 |  1  |  1  |  1  |   1   |  1   |  1  |     0     |
| 文本 2 |  1  |  1  |  0  |   0   |  1   |  1  |     1     |

</div>

<!-- slide vertical=true data-notes="" -->

##### 特征提取 以文本为例

---

<span class="blue">词袋模型</span> (bag-of-words)：文本是单词的集合，单词独立无序

- 所有文本全部$d$个不同的单词构成词典，每个文本提取$d$个特征
- 若词典第$i$个词在当前文本中出现了$k$次，则其第$i$个特征为$k$

```python {.line-numbers .top0 .left4 highlight=[4-5]}
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

document1 = "I have a pen, I have an apple, apple pen."
document2 = "I have a pen, I have pineapple, pineapple pen."

cv = CountVectorizer(lowercase=False, token_pattern='\w+')
model = cv.fit_transform([document1, document2])
pd.DataFrame(model.toarray(), columns=cv.get_feature_names_out())
```

<div class="threelines column1-border-right-solid column1-bold head-highlight-1 tr-hover top-4">

|  词典  |  I  |  a  | an  | apple | have | pen | pineapple |
| :----: | :-: | :-: | :-: | :---: | :--: | :-: | :-------: |
| 文本 1 |  2  |  1  |  1  |   2   |  2   |  2  |     0     |
| 文本 2 |  2  |  1  |  0  |   0   |  2   |  2  |     2     |

</div>

<!-- slide vertical=true data-notes="这里要说一下啥是 l1 l2" -->

##### 特征提取 以文本为例

---

<span class="blue">词频 - 逆文本频率</span>特征：对当前文本重要的单词必然

- 在当前文本中出现的频率高，即词频 (term frequency, tf) 高
- 在其他文本中出现的频率低，即逆文本频率 (inverse document frequency, idf) 高

tf = 单词在当前文本中出现的次数 / 当前文本的总词数<br>idf = ln ((全部文本数 + C) / (包含该词的总文本数 + C)) + 1</span>

- C = 0，若词典包含从未在任何文本中出现的词，会有分母为零的问题
- C = 1，sklearn 默认的平滑版本，相当于额外有一个包含所有词的文本

tf - idf 特征 = normalize (tf × idf)，即将 tf 和 idf 相乘后再标准化

- $\ell_1$标准化，tf × idf / sum (tf × idf)，即线性变换成概率分布
- $\ell_2$标准化，tf × idf / sqrt(sum ([tf × idf]^2))，即线性变换成模 1 向量

<!-- slide vertical=true data-notes="" -->

##### 特征提取 以文本为例

---

```python {.line-numbers .top0 .left4 highlight=[4-5]}
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

document1 = "I have a pen, I have an apple, apple pen."
document2 = "I have a pen, I have pineapple, pineapple pen."

tv = TfidfVectorizer(lowercase=False, token_pattern='\w+',
                     norm='l1', smooth_idf=False) # l1归一化 idf不平滑
model = tv.fit_transform([document1, document2])
pd.DataFrame(model.toarray(), columns=cv.get_feature_names_out())
```

<div class="threelines row3-border-top-dashed row3-border-bottom-dashed column1-border1-right-dashed-head row1-column1-border1-right-dashed row3-column1-border1-right-dashed row4-column1-border1-right-dashed head-highlight-1 tr-hover top-4">

|   词典   |     I      |     a      |     an     |   apple    |    have    |    pen     | pineapple  |
| :------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: |
|    tf    |   2 / 10   |   1 / 10   |   1 / 10   |   2 / 10   |   2 / 10   |   2 / 10   |     0      |
|    ^     |   2 / 9    |   1 / 9    |     0      |     0      |   2 / 9    |   2 / 9    |   2 / 9    |
|   idf    | ln (1) + 1 | ln (1) + 1 | ln (2) + 1 | ln (2) + 1 | ln (1) + 1 | ln (1) + 1 | ln (2) + 1 |
| tf - idf |  0.165571  |  0.082785  |  0.140168  |  0.280335  |  0.165571  |  0.165571  |  0.000000  |
|    ^     |  0.192561  |  0.096281  |  0.000000  |  0.000000  |  0.192561  |  0.192561  |  0.326035  |

</div>

<!-- slide data-notes="" -->

##### 特征 离散类别 → 数值

---

<div class="threelines column9-border-right-solid head-highlight-1 tr-hover">

| 编号 | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 密度  | 含糖率 | 好瓜 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :---: | :----: | :--: |
|  1   | 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.697 | 0.460  |  是  |
|  2   | 乌黑 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 0.774 | 0.376  |  是  |
|  3   | 乌黑 | 稍蜷 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 0.666 | 0.091  |  否  |
|  4   | 浅白 | 硬挺 | 清脆 | 模糊 | 平坦 | 硬滑 | 0.245 | 0.057  |  否  |

</div>

- 序数编码 (ordinal encoding)：清晰 - 0、稍糊 - 1、模糊 - 2，<span class="blue">需类别特征本身有序</span>，否则若青绿 - 0、乌黑 - 1、浅白 - 2，为何 | 青绿 - 浅白 | > | 乌黑 - 浅白 | ？
- 独热编码 (one-hot encoding)：青绿 - 001、乌黑 - 010、浅白 - 100，一碗水端平，所有取值距离相等，但若取值很多码会很长，且不适应动态出现的新取值
- 哈希编码 (hash encoding)：用哈希函数将任意输入映射到有限整数范围，码长固定，也能适应动态出现的新取值，但可能存在信息丢失

<!-- slide vertical=true data-notes="" -->

##### 特征独热编码

---

```python {.line-numbers .top0 .left2}
import numpy as np
from sklearn.preprocessing import LabelBinarizer, OneHotEncoder

X = np.array([
    [1, '青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', 0.697, 0.460],
    [2, '乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', 0.774, 0.376],
    [3, '乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑', 0.666, 0.091],
    [4, '浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', 0.245, 0.057],])
y = np.array(['是', '是', '否', '否']) # 类别标记只有两种取值
LabelBinarizer().fit_transform(y).squeeze()
[1 1 0 0]

enc = OneHotEncoder()
enc.fit_transform(X[:, 1:7]).toarray() # 对6个类别特征采用独热编码
[[0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1.]
 [1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]]

enc.get_feature_names_out() # 独热编码对应的原始特征
['x0_乌黑' 'x0_浅白' 'x0_青绿' 'x1_硬挺' 'x1_稍蜷' 'x1_蜷缩' 'x2_沉闷' 'x2_浊响'
 'x2_清脆' 'x3_模糊' 'x3_清晰' 'x3_稍糊' 'x4_凹陷' 'x4_平坦' 'x4_稍凹' 'x5_硬滑']
```

<!-- slide data-notes="有时会因为特殊原因，特征不是完整的，比如医院的病人数据，病人不可能把你的所有检查都来一遍，都是能省则省 <br><br> 本来机器学习的目的是要找到特征到类别标记的映射，对于缺失特征，可以将其先看作要预测的类别标记，先学一个无缺失特征到有缺失特征的映射，利用这个映射先将缺失的特征都补上，然后再学习类别标记 <br><br> 比如图里这个例子，含糖率有缺失，我就先用除含糖率外的特征学习一个到含糖率的映射，这是一个回归问题" -->

##### 特征缺失处理

---

<div class="threelines column9-border-right-solid head-highlight-1 tr-hover bottom-2">

| 编号 | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 密度  | 含糖率 | 好瓜 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :---: | :----: | :--: |
|  1   | 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.697 |   -    |  是  |
|  2   | 乌黑 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 0.774 | 0.376  |  是  |
|  3   | 乌黑 | 稍蜷 | 沉闷 |  -   | 稍凹 | 硬滑 | 0.666 | 0.091  |  否  |
|  4   | 浅白 | 硬挺 | 清脆 | 模糊 | 平坦 | 硬滑 | 0.245 | 0.057  |  否  |

</div>

删除：直接删除有特征缺失的样本，简单粗暴，信息损失

补全：

- 用未缺失该特征的样本计算平均数、中位数、众数填充，引入噪声？
- 用没有缺失的特征<span class="blue">学习并预测</span>缺失特征的取值，若两者之间无关？
- 将“缺失”本身作为一种特征取值

忽略：采用对缺失特征不敏感的学习模型，如决策树

<!-- slide vertical=true data-notes="" -->

##### 特征缺失处理

---

```python {.line-numbers .top0 .left2}
import numpy as np
from sklearn.impute import SimpleImputer

X = np.array([
    [1, '青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', 0.697, np.nan],
    [2, '乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', 0.774, 0.376],
    [3, '乌黑', '稍蜷', '沉闷', '-', '稍凹', '硬滑', 0.666, 0.091],
    [4, '浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', 0.245, 0.057],
])

imp_mean = SimpleImputer(strategy='mean')
imp_mean.fit_transform(X[:, 7:9]) # 用均值填充
[[0.697    0.17466667],
 [0.774    0.376     ],
 [0.666    0.091     ],
 [0.245    0.057     ]]

imp_median = SimpleImputer(strategy='median')
imp_median.fit_transform(X[:, 7:9]) # 用中位数填充
[[0.697    0.091],
 [0.774    0.376],
 [0.666    0.091],
 [0.245    0.057]]
```

<!-- slide vertical=true data-notes="" -->

##### 特征缺失处理

---

```python {.line-numbers .top0 .left2}
import numpy as np
from sklearn.impute import SimpleImputer

X = np.array([
    [1, '青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', 0.697, np.nan],
    [2, '乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', 0.774, 0.376],
    [3, '乌黑', '稍蜷', '沉闷', '-', '稍凹', '硬滑', 0.666, 0.091],
    [4, '浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', 0.245, 0.057],
])

imp_frequent = SimpleImputer(missing_values='-', strategy='most_frequent')
imp_frequent.fit_transform(X[:, 1:7].astype('object')) # 用众数填充
[['青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑'],
 ['乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑'],
 ['乌黑', '稍蜷', '沉闷', '清晰', '稍凹', '硬滑'],
 ['浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑']]

imp_iter = IterativeImputer()
imp_iter.fit_transform(X[:, 7:9]) # 回归器默认采用BayesianRidge
[[0.697    0.20908713],
 [0.774    0.376     ],
 [0.666    0.091     ],
 [0.245    0.057     ]]
```

<!-- slide data-notes="" -->

##### 特征标准化

---

也称归一化，旨在<span class="blue">消除不同特征间的量纲影响</span>

离差标准化：将原始特征线性变换到 [0, 1] 区间

$$
\begin{align*}
    x \leftarrow \frac{x - x_\min}{x_\max - x_\min} \in [0,1]
\end{align*}
$$

最大值标准化：除以该特征的绝对值最大值

$$
\begin{align*}
    x \leftarrow \frac{x}{\max_{i \in [m]} |x_i|} \in [-1,1]
\end{align*}
$$

标准差标准化：经过处理的特征近似符合标准正态分布$\Ncal(0,1)$

$$
\begin{align*}
    x \leftarrow \frac{x - \mu}{\sigma}, \quad x \leftarrow \frac{x - x_{\text{median}}}{\sum_{i \in [m]} |x_i - x_{\text{median}}| / m}
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 离差与最大值标准化

---

```python {.line-numbers .top0 .left2}
import numpy as np
from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler

X = np.array([
    [1, '青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', 0.697, 0.460],
    [2, '乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', 0.774, 0.376],
    [3, '乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑', 0.666, 0.091],
    [4, '浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', 0.245, 0.057],
])

MinMaxScaler().fit_transform(X[:, 7:9]) # 最大值变成1 同时 最小值变成0
[[0.85444234    1.        ],
 [1.            0.79156328],
 [0.79584121    0.08436725],
 [0.            0.        ]]

MaxAbsScaler().fit_transform(X[:, 7:9]) # 最大值变成1
[[0.9005168     1.        ],
 [1.            0.8173913 ],
 [0.86046512    0.19782609],
 [0.31653747    0.12391304]]
```

<!-- slide vertical=true data-notes="" -->

##### 标准差标准化

---

```python {.line-numbers .top0 .left2}
import numpy as np
from sklearn.preprocessing import scale

X = np.array([
    [1, '青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', 0.697, 0.460],
    [2, '乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', 0.774, 0.376],
    [3, '乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑', 0.666, 0.091],
    [4, '浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', 0.245, 0.057],
])

x = scale(X[:, 7:9])
x
[[ 0.49236904     1.22314674]
 [ 0.86589038     0.74303307]
 [ 0.34199032    -0.88592404]
 [-1.70024974    -1.08025576]]

x.mean(axis=0) # 均值接近0
[-1.11022302e-16    -1.66533454e-16]

x.std(axis=0) # 标准差为1
[1.    1.]
```
