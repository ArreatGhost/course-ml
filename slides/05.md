---
presentation:
  margin: 0
  center: false
  transition: "convex"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## 感知机

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide vertical=true data-notes="" -->

##### 大纲

---

@import "../vega/outline.json" {as="vega" .top-2}

<!-- slide data-notes="" -->

##### 剑宗气宗 路线之争

---

符号学派：明确的概念表示

连接学派：“黑箱”模型

- 从知识获取的角度来说，连接学派有先天性的缺点
- 有高效的反向传播 (BP) 算法，实际挺好用
- 超参数巨多且设置缺乏理论指导，全靠手工“调参”
- 显著降低使用者门槛，为机器学习技术走向工程实践带来便利

<!-- slide vertical=true data-notes="" -->

##### 发展历史

---

@import "../mermaid/nn.mermaid" {.center .top-3 .bottom-2}

- 八十年代红极一时：x86 系列 CPU 和内存条技术较七十年代显著提高
- 近十年梅开二度：大数据防止过拟合，显卡等计算设备性能显著提升

<!-- slide data-notes="" -->

##### M-P 神经元模型

---

感知机的基本结构单元为{==神经元==} (neuron)

- 接收来自$d$个其它神经元传来的输入信号$x_1, \ldots, x_d$
- 加权输入总和$\sum_{i \in [d]} w_i x_i$与阈值$b$进行比较
- 通过{==激活函数==} (activation function) $h(\cdot)$输出

@import "../tikz/neuron.svg" {.width45 .left10per .top5per}

<!-- slide vertical=true data-notes="" -->

##### 激活函数

---

<div class="top2"></div>

阶跃函数：不连续、不光滑

$$
\begin{align*}
    \quad \sgn(x) = \begin{cases} 1, & x \ge 0 \\ 0, & x < 0 \end{cases}
\end{align*}
$$

对数几率函数

$$
\begin{align*}
    \quad \logistic(x) = \frac{1}{1 + \exp(-x)}
\end{align*}
$$

符号函数$\sign(x) = 2 \cdot \sgn (x) - 1 = \begin{cases} 1, & x \ge 0 \\ -1, & x < 0 \end{cases}$

双曲正切函数$\tanh(x) = 2 \cdot \logistic(2x) - 1 = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}$

@import "../python/activation.svg" {.width45 .left50per .top-56per}

<!-- slide data-notes="" -->

##### 感知机

---

由输入层、输出层两层神经元组成

$$
\begin{align*}
    y & = \sign (\wv^\top \xv - b) \\
    & = \sign \left( \begin{bmatrix} \wv \\ b \end{bmatrix}^\top \begin{bmatrix} \xv \\ -1 \end{bmatrix} \right)
\end{align*}
$$

<div class="top-4"></div>

不失一般性，今后我们省略阈值$b$

<div class="top2"></div>

几何解释：

- $\wv^\top \xv$是$\Rbb^d$中以$\wv$为法向量的超平面 (hyperplane)，将空间一分为二
- 位于超平面两侧的样本分别被预测为正类和负类

<div class="top2"></div>

我的批注 感知机属于{==线性分类器==} (linear classifier) 的范畴

@import "../tikz/perceptron.svg" {.width40 .left56per .top-52per}

<!-- slide vertical=true data-notes="" -->

##### 感知机学习算法

---

输入：训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]} \in (\Rbb^d \times \{ \pm 1 \})^m$，学习率$\eta > 0$<br>输出：$\wv$

1. 初始化$\wv_0 = \zerov$，更新次数$t = 0$
2. {==while==} 训练集中存在误分类点 {==do==}
3. &emsp;&emsp;获取样本$(\xv_i, y_i)$
4. &emsp;&emsp;{==if==} $(\xv_i, y_i)$被误分类，即$y_i \wv_t^\top \xv_i \le 0$ {==then==}
5. &emsp;&emsp;&emsp;&emsp;$\wv_{t+1} \leftarrow \wv_t + \eta y_i \xv_i$
6. &emsp;&emsp;&emsp;&emsp;$t \leftarrow t + 1$

<div class="top2"></div>

若样本$(\xv_i, y_i)$被误分类，则更新后对其预测会有所改善

$$
\begin{align*}
    \qquad y_i \wv_{t+1}^\top \xv_i = y_i (\wv_t + \eta y_i \xv_i)^\top \xv_i = y_i \wv_t^\top \xv_i + \eta \|\xv_i\|_2^2 > y_i \wv_t^\top \xv_i
\end{align*}
$$

<div class="top-3"></div>

再蠢的人多错几次后也能记住教训

<!-- slide data-notes="" -->

##### <span style="font-weight:900">Novikoff</span> 定理

---

Novikoff 定理：设训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]} \in (\Rbb^d \times \{ \pm 1 \})^m$，若

- 存在$r > 0$对$\forall i \in [m]$有$\|\xv_i\| \le r$，即{==$D$被半径为$r$的球包住==}
- 存在$\rho>0$和$\|\vv\|=1$对$\forall i \in [m]$有$y_i \vv^\top \xv_i \ge \rho$，即{==以间隔$\rho$线性可分==}

则感知机更新次数$M \le r^2/\rho^2$

<div class="top2"></div>

三点说明：

- 这个界与维度$d$无关，与学习率$\eta$也无关，因此一般将其定为$1$
- 这个界是紧的，即存在最坏情况使得感知机的更新次数$M = r^2/\rho^2$
- 只要样本线性可分，感知机都会在有限步内中止，但当间隔$\rho$很小时，收敛可能会很慢，存在一些极端的情况，更新次数达到$\Omega(2^m)$

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">Novikoff</span> 定理证明

---

证明：设$I = \{ i_1, i_2, \ldots, i_M \}$是感知机更新时的样本下标集合

$$
\begin{align*}
    M \rho & \le \sum_{t \in [M]} y_{i_t} \vv^\top \xv_{i_t} \le \|\vv\| \left\| \sum_{t \in [M]} y_{i_t} \xv_{i_t} \right\| = \left\| \sum_{t \in [M]} \frac{\wv_t - \wv_{t-1}}{\eta} \right\| = \left\| \frac{\wv_M}{\eta} \right\| \\
    & = \sqrt{\frac{\| \wv_M \|^2}{\eta^2}} = \sqrt{\sum_{t \in [M]} \frac{\| \wv_t \|^2 - \| \wv_{t-1} \|^2}{\eta^2}} \quad \longleftarrow \wv_t = \wv_{t-1} + \eta y_{i_t} \xv_{i_t} \\
    & = \sqrt{\sum_{t \in [M]} \frac{2 \eta y_{i_t} \wv_{t-1}^\top \xv_{i_t} + \eta^2 \| \xv_{i_t} \|^2}{\eta^2}} \le \sqrt{\sum_{t \in [M]} \| \xv_{i_t} \|^2} \le \sqrt{M r^2} \\
    & \Longrightarrow M \le \frac{r^2}{\rho^2}
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">Novikoff</span> 定理的紧性

---

设$m$个样本是$\Rbb^m$的单位向量，则前$m$轮感知机预测全错，连续更新$m$次后得到$\wv_m = \eta \sum_{t \in [m]} y_t \xv_t$，由样本间的正交性可知

$$
\begin{align*}
    \qquad y_i \wv_m^\top \xv_i = y_i \eta \sum_{t \in [m]} y_t \xv_t^\top \xv_i = \eta \| \xv_i \|^2 = \eta > 0
\end{align*}
$$

<div class="top-4"></div>

即$\wv_m$可将所有样本正确分类，总更新次数为$m$

注意$r^2 = 1$，剩下只需说明间隔$\rho \le 1 / \sqrt{m}$，用反证法，假设存在单位向量$\vv = [v_1; \ldots; v_m]$使得间隔$\rho  > 1 / \sqrt{m}$，则

$$
\begin{align*}
    \qquad v_1 = \vv^\top \xv_1 \ge \rho > \frac{1}{\sqrt{m}}, \ldots, v_m  = \vv^\top \xv_m \ge \rho > \frac{1}{\sqrt{m}}
\end{align*}
$$

<div class="top-4"></div>

故$\|\vv\|^2 = v_1^2 + \cdots + v_m^2 > \frac{1}{m} + \cdots + \frac{1}{m} = 1$，与$\vv$是单位向量矛盾

<!-- slide vertical=true data-notes="" -->

##### 感知机的收敛速度

---

设$\Rbb^m$中的$m$个样本序列为

$$
\begin{align*}
    \begin{matrix}
        \xv_1 = [ & 1,      & 0,      & 0,      & 0,          & 0, & \cdots, & 0 & ], & y_1 = 1          \\
        \xv_2 = [ & 1,      & -1,     & 0,      & 0,          & 0, & \cdots, & 0 & ], & y_2 = -1         \\
        \xv_3 = [ & -1,     & -1,     & 1,      & 0,          & 0, & \cdots, & 0 & ], & y_3 = 1          \\
        \xv_4 = [ & 1,      & 1,      & 1,      & -1,         & 0, & \cdots, & 0 & ], & y_4 = -1         \\
        \xv_5 = [ & -1,     & -1,     & -1,     & -1,         & 1, & \cdots, & 0 & ], & y_5 = 1          \\
                    & \vdots                                                                               \\
        \xv_i = [ & (-1)^i, & \cdots, & (-1)^i, & (-1)^{i+1}, & 0, & \cdots, & 0 & ], & y_i = (-1)^{i+1}
    \end{matrix}
\end{align*}
$$

易知$y_i \xv_i = [\underbrace{-1, \ldots, -1}_{i-1\text{个}}, 1, 0, \ldots, 0]$，注意$1$只出现在第$i$位上

欲使$\xv_1$分类正确，需要有$w_1 > 0$，通过加一次$\eta y_1 \xv_1$即可

<!-- slide vertical=true data-notes="" -->

##### 感知机的收敛速度

---

欲使$\xv_2$分类正确，至少需要有$w_2 > 0$，同样通过加一次$\eta y_2 \xv_2$可以使得$w_2 > 0$，但这个操作同时会将$w_1$减小$\eta$，因此要想$w_1$不发生变化，加一次$\eta y_2 \xv_2$后需要跟着再加一次$\eta y_1 \xv_1$；

欲使$\xv_3$分类正确，至少需要有$w_3 > 0$，同样通过加一次$\eta y_3 \xv_3$可以使得$w_3 > 0$，但这个操作同时会将$w_1$和$w_2$减小$\eta$，因此加一次$\eta y_3 \xv_3$后需要跟着再加一次$\eta y_1 \xv_1$和一次$\eta y_2 \xv_2$，而加一次$\eta y_2 \xv_2$又得再加一次$\eta y_1 \xv_1$；

如此不难发现，设欲使$w_i > 0$且同时不改变其他$w_j(j \neq i)$所需的更新次数为$a_i$，由前面的分析可知

$$
\begin{align*}
    a_1 & = 1 = 2^0                       \\
    a_2 & = 1 + a_1 = 2 = 2^1             \\
    a_3 & = 1 + a_2 + a_1 = 4 = 2^2       \\
    a_4 & = 1 + a_3 + a_2 + a_1 = 8 = 2^3 \\
        & \vdots                          \\
    a_i & = 1 + a_{i-1} + \cdots + a_1
\end{align*}
$$

由数学归纳法不难证明$a_i = 2^{i-1}$，因此光是将$\wv$的全部分量变为正数所需的更新次数就至少为$2^0 + 2^1 + \cdots + 2^{n-1} = 2^n$，而将全部样本分类正确的要求更苛刻，因此所需的更新次数更多，所以为$\Omega(2^n)$。

<!-- slide data-notes="" -->

##### 感知机

---

实现与、或、非运算，设$x_1,x_2$为布尔变量

- 与：$\sign(x_1 + x_2 - 1.5) = \begin{cases} 1, & x_1 = x_2 = 1 \\ -1, & \ow \end{cases}$
- 或：$\sign(x_1 + x_2 - 0.5) = \begin{cases} -1, & x_1 = x_2 = 0 \\ 1, & \ow \end{cases}$
- 非：$\sign(- x_1 + 0.5) = \begin{cases} -1, & x_1 = 1 \\ 1, & x_1 = -1 \end{cases}$

再$(\sign(\cdots) + 1) / 2$就
