---
presentation:
  margin: 0
  center: false
  transition: "convex"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## 对数几率回归

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide vertical=true data-notes="" -->

##### 大纲

---

@import "../vega/outline.json" {as="vega" .top-2}

<!-- slide data-notes="" -->

##### 对数几率函数

---

感知机采用符号函数作为激活函数

$$
\begin{align*}
    \quad y = \sign(\wv^\top \xv) = \begin{cases} +1, & \wv^\top \xv \ge 0 \\ -1, & \wv^\top \xv < 0 \end{cases}
\end{align*}
$$

常用对(数几)率函数替代符号函数

$$
\begin{align*}
    \quad y = \sigma(\wv^\top \xv) = \frac{1}{1 + \exp(-\wv^\top \xv)}
\end{align*}
$$

对率函数$\sigma(z)$有很好的数学性质：

$$
\begin{align*}
    \quad & \sigma(z) = \frac{1}{1 + \exp(-z)} = \frac{\exp(z)}{1 + \exp(z)} = 1 - \sigma(- z) \\
    \quad & \nabla_z \sigma(z) = \nabla_z \frac{1}{1 + \exp(-z)} = \frac{1}{1 + \exp(-z)} \frac{\exp(-z)}{1 + \exp(-z)} = \sigma(z) (1 - \sigma(z))
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 对数几率回归

---

{==对率函数输出是连续的$[0,1]$==}，可视为{==后验概率==}估计$\Pr(y = +1 | \xv)$

$$
\begin{align*}
    \qquad \Pr(y = +1 | \xv) &  = \sigma(\wv^\top \xv) = \frac{1}{1 + \exp(-\wv^\top \xv)} \\
    \Pr(y = -1 | \xv) & = 1 - \sigma(\wv^\top \xv) = \frac{\exp(-\wv^\top \xv)}{1 + \exp(-\wv^\top \xv)} = \sigma(-\wv^\top \xv)
\end{align*}
$$

<div class="top-4"></div>

两式相除再取对数可得对(数几)率回归 (<u>l</u>ogistic <u>r</u>egression, LR)

$$
\begin{align*}
    \qquad \wv^\top \xv = \ln \frac{\Pr(y = +1 | \xv)}{\Pr(y = -1 | \xv)} = \ln \frac{\sigma(\wv^\top \xv)}{1 - \sigma(\wv^\top \xv)}
\end{align*}
$$

- 正类概率与负类概率的比值称为{==几率==} (odds)
- 对率回归，顾名思义就是{==用线性函数拟合几率的对数==} (logit)
- 有人将其译为逻辑(斯蒂)回归，但逻辑 (logic) 与 (logistic) 相去甚远
- 虽然名字里有回归，实际是个分类模型

<!-- slide data-notes="" -->

##### 对率回归求解

---

<div class="top2"></div>

$$
\begin{align*}
    \qquad \Pr(y = +1 | \xv) = \sigma(\wv^\top \xv), \quad \Pr(y = -1 | \xv) = \sigma(-\wv^\top \xv)
\end{align*}
$$

<div class="top-2"></div>

不难发现后验概率可紧凑地写为$\Pr(y | \xv) = \sigma(y \wv^\top \xv)$

<div class="top2"></div>

设训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]}$，则对数似然函数为

$$
\begin{align*}
    \qquad \ell(\wv) & = \ln \prod_{i \in [m]} \Pr(y_i | \xv_i) = \sum_{i \in [m]} \ln \sigma(y_i \wv^\top \xv_i) \\
    & = \sum_{i \in [m]} \ln \frac{1}{1 + \exp(- y_i \wv^\top \xv_i)}
\end{align*}
$$

<div class="top-2"></div>

根据极大似然法易知

$$
\begin{align*}
    \qquad \wv^\star = \argmin_{\wv} \sum_{i \in [m]} \ln (1 + \exp(- y_i \wv^\top \xv_i))
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 对率回归求解

---

对率回归形式化的优化问题为

$$
\begin{align*}
    \quad \min_{\wv} \sum_{i \in [m]} \ln (1 + \exp(- y_i \wv^\top \xv_i)) = - \sum_{i \in [m]} \ln \sigma(y_i \wv^\top \xv_i)
\end{align*}
$$

<div class="top-2"></div>

目标函数的梯度$\gv$和海森矩阵$\Hv$分别为

$$
\begin{align*}
    \quad \gv & = - \sum_{i \in [m]} \frac{\sigma(y_i \wv^\top \xv_i) (1 - \sigma(y_i \wv^\top \xv_i))}{\sigma(y_i \wv^\top \xv_i)} y_i \xv_i = \sum_{i \in [m]} (\sigma(y_i \wv^\top \xv_i) - 1) y_i \xv_i \\
    \Hv & = \sum_{i \in [m]} (\sigma(y_i \wv^\top \xv_i)) (1 - \sigma(y_i \wv^\top \xv_i)) \xv_i \xv_i^\top
\end{align*}
$$

<div class="top-2"></div>

有了梯度，就可以用一阶优化算法如{==梯度下降法==}进行求解

有了海森矩阵，就可以用二阶优化算法如{==牛顿法==}进行求解

<!-- slide vertical=true data-notes="" -->

##### 对率回归求解

---

若标记集合$\Ycal = \{ 0,1 \}$，记

- $\Pr(y = 1 | \xv) = \sigma(\wv^\top \xv)$
- $\Pr(y = 0 | \xv) = 1 - \sigma(\wv^\top \xv) = \sigma(-\wv^\top \xv)$

<div class="top2"></div>

设训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]}$，则对数似然函数为

$$
\begin{align*}
    \qquad \ell(\wv) & = \ln \prod_{i \in [m]} \Pr(y_i | \xv_i) = \sum_{i \in [m]} \ln \class{blue}{\sigma(\wv^\top \xv_i)^{y_i} \sigma(-\wv^\top \xv_i)^{1 - y_i}} \\
    & = \sum_{i \in [m]} (y_i \ln \sigma(\wv^\top \xv_i) + (1 - y_i) \ln \sigma(-\wv^\top \xv_i)) \\
    & = \sum_{i \in [m]} \left( y_i \ln \frac{\sigma(\wv^\top \xv_i)}{1 - \sigma(\wv^\top \xv_i)} + \ln \sigma(-\wv^\top \xv_i) \right) \\
    & = \sum_{i \in [m]} (y_i \wv^\top \xv_i - \ln (1 + \exp(\wv^\top \xv_i)))
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 对率回归求解

---

根据极大似然法，优化问题为

$$
\begin{align*}
    \quad \min_{\wv} \sum_{i \in [m]} (\ln (1 + \exp(\wv^\top \xv_i)) - y_i \wv^\top \xv_i) = \sum_{i \in [m]} (- \ln \sigma(-\wv^\top \xv_i) - y_i \wv^\top \xv_i)
\end{align*}
$$

<div class="top-2"></div>

目标函数的梯度和海森矩阵分别为

$$
\begin{align*}
    \quad \gv & = \sum_{i \in [m]} \left( - \frac{\sigma(-\wv^\top \xv_i) (1 - \sigma(-\wv^\top \xv_i))}{\sigma(-\wv^\top \xv_i)} (-\xv_i) - y_i \xv_i \right) \\
    & = \sum_{i \in [m]} (\sigma(\wv^\top \xv_i) - y_i) \xv_i \\[4pt]
    \Hv & = \sum_{i \in [m]} \sigma(\wv^\top \xv_i) (1 - \sigma(\wv^\top \xv_i)) \xv_i \xv_i^\top
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 对率回归求解

---

<div class="threelines column1-border-right-solid head-highlight-1 tr-hover">

| &emsp;  |                                                 >                                                 |                                      对率回归求解                                       |
| :-----: | :-----------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------: |
| $\Ycal$ |                                           $\{ \pm 1 \}$                                           |                                       $\{ 0,1 \}$                                       |
|  _obj_  |                      $\sum_{i \in [m]} \ln (1 + \exp(- y_i \wv^\top \xv_i))$                      |        $\sum_{i \in [m]} (\ln (1 + \exp(\wv^\top \xv_i)) - y_i \wv^\top \xv_i)$         |
|  $\gv$  |                   $\sum_{i \in [m]} (\sigma(y_i \wv^\top \xv_i) - 1) y_i \xv_i$                   |                 $\sum_{i \in [m]} (\sigma(\wv^\top \xv_i) - y_i) \xv_i$                 |
|  $\Hv$  | $\sum_{i \in [m]} (\sigma(y_i \wv^\top \xv_i)) (1 - \sigma(y_i \wv^\top \xv_i)) \xv_i \xv_i^\top$ | $\sum_{i \in [m]} \sigma(\wv^\top \xv_i) (1 - \sigma(\wv^\top \xv_i)) \xv_i \xv_i^\top$ |

</div>
