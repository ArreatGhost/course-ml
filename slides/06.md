---
presentation:
  margin: 0
  center: false
  transition: "convex"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## 对数几率回归

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide vertical=true data-notes="" -->

##### 大纲

---

@import "../vega/outline.json" {as="vega" .top-2}

<!-- slide data-notes="" -->

##### 对数几率函数

---

感知机采用符号函数作为激活函数

$$
\begin{align*}
    \quad y = \sign(\wv^\top \xv) = \begin{cases} +1, & \wv^\top \xv \ge 0 \\ -1, & \wv^\top \xv < 0 \end{cases}
\end{align*}
$$

常用对(数几)率函数替代符号函数

$$
\begin{align*}
    \quad y = \sigma(\wv^\top \xv) = \frac{1}{1 + \exp(-\wv^\top \xv)}
\end{align*}
$$

对率函数$\sigma(z)$有很好的数学性质：

$$
\begin{align*}
    \quad & \sigma(z) = \frac{1}{1 + \exp(-z)} = \frac{\exp(z)}{1 + \exp(z)} = 1 - \sigma(- z) \\
    \quad & \nabla_z \sigma(z) = \nabla_z \frac{1}{1 + \exp(-z)} = \frac{1}{1 + \exp(-z)} \frac{\exp(-z)}{1 + \exp(-z)} = \sigma(z) (1 - \sigma(z))
\end{align*}
$$

@import "../python/logistic-func-plot.svg" {.width40 .left55per .top-56per}

<!-- slide vertical=true data-notes="" -->

##### 对数几率回归

---

{==对率函数输出是连续的$[0,1]$==}，可视为{==后验概率==}估计$\Pr(y = +1 | \xv)$

$$
\begin{align*}
    \qquad \Pr(y = +1 | \xv) &  = \sigma(\wv^\top \xv) = \frac{1}{1 + \exp(-\wv^\top \xv)} \\
    \Pr(y = -1 | \xv) & = 1 - \sigma(\wv^\top \xv) = \frac{\exp(-\wv^\top \xv)}{1 + \exp(-\wv^\top \xv)} = \sigma(-\wv^\top \xv)
\end{align*}
$$

<div class="top-4"></div>

两式相除再取对数可得对(数几)率回归 (<u>l</u>ogistic <u>r</u>egression, LR)

$$
\begin{align*}
    \qquad \wv^\top \xv = \ln \frac{\Pr(y = +1 | \xv)}{\Pr(y = -1 | \xv)} = \ln \frac{\sigma(\wv^\top \xv)}{1 - \sigma(\wv^\top \xv)}
\end{align*}
$$

- 正类概率与负类概率的比值称为{==几率==} (odds)
- 对率回归，顾名思义就是{==用线性函数拟合几率的对数==} (logit)
- 有人将其译为逻辑(斯蒂)回归，但逻辑 (logic) 与 (logistic) 相去甚远
- 虽然名字里有回归，实际是个分类模型

<!-- slide data-notes="" -->

##### 对率回归求解

---

<div class="top2"></div>

$$
\begin{align*}
    \qquad \Pr(y = +1 | \xv) = \sigma(\wv^\top \xv), \quad \Pr(y = -1 | \xv) = \sigma(-\wv^\top \xv)
\end{align*}
$$

<div class="top-2"></div>

不难发现后验概率可紧凑地写为$\Pr(y | \xv) = \sigma(y \wv^\top \xv)$

<div class="top2"></div>

设训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]}$，则对数似然函数为

$$
\begin{align*}
    \qquad \ell(\wv) & = \ln \prod_{i \in [m]} \Pr(y_i | \xv_i) = \sum_{i \in [m]} \ln \sigma(y_i \wv^\top \xv_i) \\
    & = \sum_{i \in [m]} \ln \frac{1}{1 + \exp(- y_i \wv^\top \xv_i)}
\end{align*}
$$

<div class="top-2"></div>

根据极大似然法易知

$$
\begin{align*}
    \qquad \wv^\star = \argmin_{\wv} \sum_{i \in [m]} \ln (1 + \exp(- y_i \wv^\top \xv_i))
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 对率回归求解

---

对率回归形式化的优化问题为

$$
\begin{align*}
    \quad \min_{\wv} \sum_{i \in [m]} \ln (1 + \exp(- y_i \wv^\top \xv_i)) = - \sum_{i \in [m]} \ln \sigma(y_i \wv^\top \xv_i)
\end{align*}
$$

<div class="top-2"></div>

目标函数的梯度$\gv$和海森矩阵$\Hv$分别为

$$
\begin{align*}
    \quad \gv & = - \sum_{i \in [m]} \frac{\sigma(y_i \wv^\top \xv_i) (1 - \sigma(y_i \wv^\top \xv_i))}{\sigma(y_i \wv^\top \xv_i)} y_i \xv_i = \sum_{i \in [m]} (\sigma(y_i \wv^\top \xv_i) - 1) y_i \xv_i \\
    \Hv & = \sum_{i \in [m]} (\sigma(y_i \wv^\top \xv_i)) (1 - \sigma(y_i \wv^\top \xv_i)) \xv_i \xv_i^\top
\end{align*}
$$

<div class="top-2"></div>

有了梯度，就可以用一阶优化算法如{==梯度下降法==}进行求解

有了海森矩阵，就可以用二阶优化算法如{==牛顿法==}进行求解

<!-- slide vertical=true data-notes="" -->

##### 对率回归求解

---

若标记集合$\Ycal = \{ 0,1 \}$，记

- $\Pr(y = 1 | \xv) = \sigma(\wv^\top \xv)$
- $\Pr(y = 0 | \xv) = 1 - \sigma(\wv^\top \xv) = \sigma(-\wv^\top \xv)$

<div class="top2"></div>

设训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]}$，则对数似然函数为

$$
\begin{align*}
    \qquad \ell(\wv) & = \ln \prod_{i \in [m]} \Pr(y_i | \xv_i) = \sum_{i \in [m]} \ln \class{blue}{\sigma(\wv^\top \xv_i)^{y_i} \sigma(-\wv^\top \xv_i)^{1 - y_i}} \\
    & = \sum_{i \in [m]} (y_i \ln \sigma(\wv^\top \xv_i) + (1 - y_i) \ln \sigma(-\wv^\top \xv_i)) \\
    & = \sum_{i \in [m]} \left( y_i \ln \frac{\sigma(\wv^\top \xv_i)}{1 - \sigma(\wv^\top \xv_i)} + \ln \sigma(-\wv^\top \xv_i) \right) \\
    & = \sum_{i \in [m]} (y_i \wv^\top \xv_i - \ln (1 + \exp(\wv^\top \xv_i)))
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 对率回归求解

---

根据极大似然法，优化问题为

$$
\begin{align*}
    \quad \min_{\wv} \sum_{i \in [m]} (\ln (1 + \exp(\wv^\top \xv_i)) - y_i \wv^\top \xv_i) = \sum_{i \in [m]} (- \ln \sigma(-\wv^\top \xv_i) - y_i \wv^\top \xv_i)
\end{align*}
$$

<div class="top-2"></div>

目标函数的梯度和海森矩阵分别为

$$
\begin{align*}
    \quad \gv & = \sum_{i \in [m]} \left( - \frac{\sigma(-\wv^\top \xv_i) (1 - \sigma(-\wv^\top \xv_i))}{\sigma(-\wv^\top \xv_i)} (-\xv_i) - y_i \xv_i \right) \\
    & = \sum_{i \in [m]} (\sigma(\wv^\top \xv_i) - y_i) \xv_i \\[4pt]
    \Hv & = \sum_{i \in [m]} \sigma(\wv^\top \xv_i) (1 - \sigma(\wv^\top \xv_i)) \xv_i \xv_i^\top
\end{align*}
$$

<!-- slide data-notes="" -->

##### 另一个视角

---

<div class="threelines row1-column2-border1-left-solid row2-column2-border1-left-solid row3-column1-border1-left-solid row4-column2-border1-left-solid row5-column1-border1-left-solid row6-column2-border1-left-solid row7-column2-border1-left-solid row3-column1-border1-right-dashed row5-column1-border1-right-dashed column3-border-left-dashed column1-border1-right-solid-head row1-border-bottom-dashed row3-border-bottom-dashed row5-border-bottom-dashed row6-border-bottom-dashed head-highlight-1 tr-hover">

|    &emsp;    |                                            >                                            |                                      对率回归                                      |
| :----------: | :-------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------: |
|   $\Ycal$    |                                      $\{ \pm 1 \}$                                      |                                    $\{ 0,1 \}$                                     |
| 后验<br>概率 |                  $\Pr(y=+1 \big\arrowvert \xv) = \sigma(\wv^\top \xv)$                  |                $\Pr(y=1 \big\arrowvert \xv) = \sigma(\wv^\top \xv)$                |
|      ^       |                 $\Pr(y=-1 \big\arrowvert \xv) = \sigma(-\wv^\top \xv)$                  |               $\Pr(y=0 \big\arrowvert \xv) = \sigma(-\wv^\top \xv)$                |
| 优化<br>目标 |                $\min_{\wv} \sum_i \ln (1 + \exp(- y_i \wv^\top \xv_i))$                 |     $\min_{\wv} \sum_i (\ln (1 + \exp(\wv^\top \xv_i)) - y_i \wv^\top \xv_i)$      |
|      ^       |            $\min_{\wv} (- \sum_{i \in [m]} \ln \sigma(y_i \wv^\top \xv_i))$             | $\min_{\wv} \sum_{i \in [m]} (- \ln \sigma(-\wv^\top \xv_i) - y_i \wv^\top \xv_i)$ |
|    $\gv$     |                   $\sum_i (\sigma(y_i \wv^\top \xv_i) - 1) y_i \xv_i$                   |                   $\sum_i (\sigma(\wv^\top \xv_i) - y_i) \xv_i$                    |
|    $\Hv$     | $\sum_i (\sigma(y_i \wv^\top \xv_i)) (1 - \sigma(y_i \wv^\top \xv_i)) \xv_i \xv_i^\top$ |   $\sum_i \sigma(\wv^\top \xv_i) (1 - \sigma(\wv^\top \xv_i)) \xv_i \xv_i^\top$    |

</div>

以上结果是通过{==极大似然法==}导出的

用{==交叉熵==} (cross entropy) 可以导出一致的结果

<!-- slide vertical=true data-notes="" -->

##### 交叉熵

---

问题：给定分布$\qv$，如何度量分布$\pv$与它之间的差异？

定义交叉熵$H_{\qv} (\pv) \triangleq - \sum_i q_i \log p_i = \sum_i q_i \log (1/p_i)$

当$\pv = \qv$时交叉熵最小，此时交叉熵$H_{\qv} (\pv)$即为分布$\qv$的熵$H(\qv)$

<div class="top1"></div>

$$
\begin{align*}
    \qquad \min_{\pv} ~ H_{\qv} (\pv) = - \sum_i q_i \log p_i = \sum_i q_i \log (1/p_i) \quad \st ~ \sum_i p_i = 1
\end{align*}
$$

<div class="top-4"></div>

拉格朗日函数为$L = - \sum_i q_i \log p_i + \alpha (\sum_i p_i - 1)$，于是

<div class="top1"></div>

$$
\begin{align*}
    \qquad \nabla_{p_i} L = - \frac{q_i}{p_i} + \alpha = 0 \Longrightarrow q_i = \alpha p_i \Longrightarrow \sum_i q_i = \alpha \sum_i p_i \Longrightarrow \alpha = 1
\end{align*}
$$

- 取$\qv$为真实标记在类别上的分布，$\pv$为预测结果在类别上的分布
- 交叉熵可以看成一种{==损失==} (loss)，越小预测结果和真实标记越接近

<!-- slide vertical=true data-notes="" -->

##### 交叉熵损失

---

- 真实标记分布：$\qv_{\{ \pm 1 \}} = [(1+y)/2; (1-y)/2]$，$\qv_{\{ 0,1 \}} = [y; 1-y]$
- 预测结果分布：$\pv = [\sigma(\wv^\top \xv); \sigma(-\wv^\top \xv)]$

<div class="top4"></div>

$$
\begin{align*}
    \qquad H_{\qv_{\{ \pm 1 \}}} (\pv) & = - \frac{1+y}{2} \log \sigma(\wv^\top \xv) - \frac{1-y}{2} \log \sigma(-\wv^\top \xv) \\
    & = \begin{cases}
        - \log \sigma(\wv^\top \xv), & y = 1 \\
        - \log \sigma(-\wv^\top \xv), & y = -1
    \end{cases} \\
    & = - \log \sigma(y \wv^\top \xv) = \log (1 + \exp (- y \wv^\top \xv)) \\[10pt]
    H_{\qv_{\{ 0,1 \}}} (\pv) & = - y \log \sigma(\wv^\top \xv) - (1-y) \log \sigma(-\wv^\top \xv) \\
    & = -y \log \frac{\sigma(\wv^\top \xv)}{1 - \sigma(\wv^\top \xv)} - \log \sigma(-\wv^\top \xv) \\
    & = \log (1 + \exp(\wv^\top \xv)) - y \wv^\top \xv
\end{align*}
$$

<div class="top-2"></div>

我的批注 交叉熵里用的是$\log$，和前面的$\ln$只差个常数系数$\log e$

<!-- slide data-notes="" -->

##### 多分类扩展

---

设共有$c$个类，对率回归的预测结果分布为

$$
\begin{align*}
    \Pr(y=1|\xv) & = \sigma_1 (\xv) = \frac{\exp(\wv_1^\top \xv)}{\sum_{k \in [c]} \exp(\wv_i^\top \xv)} = \frac{\exp((\wv_1 - \wv_c)^\top \xv)}{1 + \sum_{k \in [c-1]} \exp((\wv_i - \wv_c)^\top \xv)} \\
    \Pr(y=2|\xv) & = \sigma_2 (\xv) = \frac{\exp(\wv_2^\top \xv)}{\sum_{k \in [c]} \exp(\wv_i^\top \xv)} = \frac{\exp((\wv_2 - \wv_c)^\top \xv)}{1 + \sum_{k \in [c-1]} \exp((\wv_i - \wv_c)^\top \xv)} \\
    & \vdots \\
    \Pr(y=c|\xv) & = \sigma_c (\xv) = \frac{\exp(\wv_c^\top \xv)}{\sum_{k \in [c]} \exp(\wv_i^\top \xv)} = \frac{1}{1 + \sum_{k \in [c-1]} \exp((\wv_i - \wv_c)^\top \xv)}
\end{align*}
$$

- 前一种写法称为 {==softmax==} 变换，二分类 logistic 激活函数的多分类推广
- 后一种写法只需$\uv_1 = \wv_1 - \wv_c, \ldots, \uv_{c-1} = \wv_{c-1} - \wv_c$这$c-1$个向量
- 若$c=2$，则$\uv_1 = \wv_1 - \wv_2$就是前面二分类对率回归中要学的$\wv$

<!-- slide vertical=true data-notes="" -->

##### 多分类对率回归

---

后验概率$\Pr(y|\xv) = \sigma_1(\xv)^{\Ibb(y=1)} \cdots \sigma_c(\xv)^{\Ibb(y=c)}$，根据极大似然估计

<div class="top1"></div>

$$
\begin{align*}
    \qquad \max_{\wv_1, \ldots, \wv_c} \sum_{i \in [m]} \ln \left( \sigma_1(\xv_i)^{\Ibb(y_i=1)} \cdots \sigma_c(\xv_i)^{\Ibb(y_i=c)} \right) = \sum_{i \in [m]} \ln \sigma_{y_i}(\xv_i)
\end{align*}
$$

真实标记分布为$[\Ibb(y = 1);\ldots;\Ibb(y = c)]$，根据最小化交叉熵损失

$$
\begin{align*}
    \qquad \min_{\wv_1, \ldots, \wv_c} \sum_{i \in [m]} \sum_{k \in [c]} \Ibb(y_i = k) \log \frac{1}{\sigma_k(\xv_i)} = \sum_{i \in [m]} \log \frac{1}{\sigma_{y_i}(\xv_i)}
\end{align*}
$$

我的批注 {==极大似然估计==}和{==最小化交叉熵损失==}再次取得了一致

<!-- slide data-notes="" -->

##### 多分类对率回归求解

---

注意

$$
\begin{align*}
    ~ \sigma_{y_i} (\xv_i) = \frac{\exp(\wv_{y_i}^\top \xv_i)}{\sum_{k \in [c]} \exp(\wv_k^\top \xv_i)}
\end{align*}
$$

<div class="top-4"></div>

于是

$$
\begin{align*}
    ~ \nabla_{\wv_{y_i}} \sigma_{y_i} (\xv_i) & = \frac{\exp(\wv_{y_i}^\top \xv_i) \xv_i \sum_{k \in [c]} \exp(\wv_k^\top \xv_i) - \exp(\wv_{y_i}^\top \xv_i) \exp(\wv_{y_i}^\top \xv_i) \xv_i}{(\sum_{k \in [c]} \exp(\wv_k^\top \xv_i))^2} \\[4pt]
    & = \sigma_{y_i} (\xv_i) \xv_i - \sigma_{y_i}^2 (\xv_i) \xv_i = \sigma_{y_i} (\xv_i) (1 - \sigma_{y_i} (\xv_i)) \xv_i \\[10pt]
    \nabla_{\wv_l} \sigma_{y_i} (\xv_i) & = \frac{- \exp(\wv_{y_i}^\top \xv_i) \exp(\wv_l^\top \xv_i) \xv_i}{(\sum_{k \in [c]} \exp(\wv_k^\top \xv_i))^2} = - \sigma_{y_i} (\xv_i) \sigma_l (\xv_i) \xv_i, \quad l \ne y_i
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 多分类对率回归求解

---

<div class="top2"></div>

$$
\begin{align*}
    \qquad & \nabla_{\wv_{y_i}} \sigma_{y_i} (\xv_i) = \class{blue}{\sigma_{y_i} (\xv_i) (1 - \sigma_{y_i} (\xv_i)) \xv_i} \\
    & \nabla_{\wv_l} \sigma_{y_i} (\xv_i) = \class{yellow}{- \sigma_{y_i} (\xv_i) \sigma_l (\xv_i) \xv_i}, \quad l \ne y_i \\[16pt]
    \quad & \nabla_{\wv_l} \left( \sum_{i \in [m]} \ln \sigma_{y_i}(\xv_i) \right) = \sum_{i \in [m]} \frac{\nabla_{\wv_l} \sigma_{y_i} (\xv_i)}{\sigma_{y_i}(\xv_i)} \\
    = & \sum_{i: y_i = l} \frac{\nabla_{\wv_l} \sigma_{y_i} (\xv_i)}{\sigma_{y_i}(\xv_i)} + \sum_{i: y_i \neq l} \frac{\nabla_{\wv_l} \sigma_{y_i} (\xv_i)}{\sigma_{y_i}(\xv_i)} \\
    = & \sum_{i: y_i = l} \frac{\class{blue}{\sigma_l (\xv_i) (1 - \sigma_l (\xv_i))\xv_i}}{\sigma_l (\xv_i)} + \sum_{i: y_i \neq l} \frac{\class{yellow}{-\sigma_{y_i} (\xv_i) \sigma_l (\xv_i) \xv_i}}{\sigma_{y_i} (\xv_i)} \\
    = & \sum_{i: y_i = l} (1 - \sigma_l (\xv_i)) \xv_i - \sum_{i: y_i \neq l} \sigma_l (\xv_i) \xv_i = \sum_{i: y_i = l} \xv_i - \sum_{i \in [m]} \sigma_l (\xv_i) \xv_i \\
    = & \sum_{i \in [m]} (\Ibb(y_i=l) - \sigma_l (\xv_i)) \xv_i
\end{align*}
$$

<!-- slide data-notes="" -->

##### <span style="font-weight:900">sklearn</span> 中的对率回归

---

```python {.line-numbers .top-1 .left4 highlight=[2,18,21-27]}
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelBinarizer, OneHotEncoder

X = np.array([[1, '周六', '吃饭', '晴天', '轻松', '清零', '精彩'], ..., [17, '周六', '吃饭', '阴天', '适中', '平缓', '精彩'],])
y = np.array(['是', '是', '是', '是', '是', '是', '是', '是', '否', '否', '否', '否', '否', '否', '否', '否', '否'])
X = OneHotEncoder().fit_transform(X[:, 1:7])
y = LabelBinarizer().fit_transform(y).squeeze()

train_index = [0, 1, 2, 5, 6, 9, 13, 14, 15, 16]
test_index = [3, 4, 7, 8, 10, 11, 12]
X_train, X_test = X[train_index, :], X[test_index, :]
y_train, y_test = y[train_index], y[test_index]

clf = LogisticRegression(penalty='none', verbose=True)  # 无正则项 输出日志
clf.fit(X_train, y_train)
clf.score(X_test, y_test)
0.7142857142857143

np.hstack((clf.predict_proba(X_test), OneHotEncoder().fit_transform(y_test[:, np.newaxis]).toarray()))
[[3.19744231e-14, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00],
 [3.45002693e-10, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00],
 [9.99999999e-01, 8.41179268e-10, 0.00000000e+00, 1.00000000e+00],
 [1.00000000e+00, 4.23642522e-19, 1.00000000e+00, 0.00000000e+00],
 [1.00000000e+00, 2.94646658e-47, 1.00000000e+00, 0.00000000e+00],
 [1.00000000e+00, 5.95930974e-17, 1.00000000e+00, 0.00000000e+00],
 [0.00000000e+00, 1.00000000e+00, 1.00000000e+00, 0.00000000e+00]]
```

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">sklearn</span> 中的对率回归

---

```python {.line-numbers .top-1 .left4 highlight=[3,14,17-19,22-24]}
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = LogisticRegression(penalty='none', verbose=True)  # 无正则项 输出日志
clf.fit(X_train, y_train)
clf.score(X_test, y_test)
1.0

clf.predict_proba(X_test)
[[1.88995213e-15, 9.99887032e-01, 1.12968186e-04],
 ...
 [9.99999994e-01, 6.37873593e-09, 6.77211243e-31]]

OneHotEncoder().fit_transform(y_test[:, np.newaxis]).toarray()
[[0., 1., 0.],
 ...
 [1., 0., 0.]]
```

<!-- slide data-notes="" -->

##### 核对率回归

---

以二分类为例，引入特征映射$\phi(\cdot)$可得原始形式的核对率回归：

<div class="top1"></div>

$$
\begin{align*}
    \quad \min_{\wv} \sum_{i \in [m]} \ln (1 + \exp(- y_i \wv^\top \xv_i)) ~ \longrightarrow ~ \min_{\wv} \sum_{i \in [m]} \ln (1 + \exp(- y_i \wv^\top \phi(\xv_i)))
\end{align*}
$$

问题：如何让模型中只出现内积$\phi(\xv_i)^\top \phi(\xv_j)$的形式？

做正交分解$\wv = \sum_{i \in [m]} \alpha_i \phi(\xv_i) + \vv$，其中对$\forall i \in [m]$有$\vv \perp \phi(\xv_i)$

注意$\kappa(\xv_j, \xv_i) = \phi(\xv_j)^\top \phi(\xv_i)$，核对率回归的对偶形式为

$$
\begin{align*}
    \quad \min_{\wv} \sum_{i \in [m]} \ln \left( 1 + \exp \left( -y_i \sum_{j \in [m]} \alpha_j \kappa(\xv_j, \xv_i) \right) \right)
\end{align*}
$$
