---
presentation:
  margin: 0
  center: false
  transition: "convex"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## 贝叶斯概率

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide data-notes="" -->

##### 贝叶斯公式

---

- 华科有$80\%$科研水平高的老师
- 讲课水平和科研水平匹配的概率是$90\%$
- 假设张老师的机器学习课你们觉得讲得有点次
- 张老师科研水平高的概率是多少？

两个随机变量：$\Theta$为张老师科研水平高，$X$为张老师讲课水平次

根据贝叶斯公式

$$
\begin{align*}
    \qquad \Pr(\Theta|X) & = \frac{\Pr(X|\Theta) \Pr(\Theta)}{\Pr(X)} = \frac{\Pr(X|\Theta) \Pr(\Theta)}{\Pr(X|\Theta) \Pr(\Theta) + \Pr(X|\neg\Theta) \Pr(\neg\Theta)} \\
    & = \frac{0.1 \times 0.8}{0.1 \times 0.8 + 0.9 \times 0.2} \approx 0.308
\end{align*}
$$

- 如果你们没上过张老师的课，只能根据经验盲猜概率是$80\%$
- 在观测到张老师讲课不好这一事实后，概率修正为$30.8\%$

<!-- slide vertical=true data-notes="" -->

##### 贝叶斯公式的新解释

---

$$
\begin{align*}
    \qquad \underbrace{\Pr(\Theta|X)}_{\text{后验}} & = \frac{\overbrace{\Pr(X|\Theta)}^{\text{似然}} \times \overbrace{\Pr(\Theta)}^{\text{先验}}}{\underbrace{\Pr(X)}_{\text{证据}}}
\end{align*}
$$

<div class="top-4"></div>

在机器学习中

- 先验 (_prior_) 对应“因”，是模型的参数 (科研水平伯努利分布的参数)
- 证据 (_evidence_) 对应 “果”，即观测到的数据
- 似然 (_likelihood_) 是由“因”到“果”的概率
- 后验 (_posterior_) 是在得到观测数据后，对模型参数的修正

<div class="top2"></div>

估计参数$\Theta$的两种方法：

- 频率主义：极大似然估计 (_<u>m</u>aximum <u>l</u>ikelihood_, ML)
- 贝叶斯主义：最大后验估计 (_<u>m</u>aximum <u>a</u> <u>p</u>osterior_, MAP)

<!-- slide vertical=true data-notes="" -->

##### 频率 _vs._ 贝叶斯

---

以抛硬币为例

- 先验$\Pr(\text{正面}) = \theta$是伯努利分布，反应硬币是否有偏
- 观测到$t$次中有$k$次正面，似然$\Pr(X | \theta) = C_t^k \theta^k (1 - \theta)^{t-k}$是二项式分布

<div class="top2"></div>

假设恰巧某次观测$X$是抛了$10$次全正，根据极大似然可知$\theta = 1$

<!-- slide data-notes="" -->

##### 贝叶斯决策论

---

- 样本空间$\Xcal \subseteq \Fbb^d$，类别标记空间为$\Ycal$
- $\Dcal$是$\Xcal \times \Ycal$上的未知概率分布，概率密度函数为$\Pr(\xv, y)$
- 损失函数$\ell: \Ycal \times \Ycal \mapsto \Rbb$

学习器$h: \Xcal \mapsto \Ycal$的泛化风险为

$$
\begin{align*}
    \qquad R_{\Dcal} (h) & = \Ebb_{(\xv,y) \sim \Dcal} [\ell(y, h(\xv))] = \iint \ell(y, h(\xv)) \Pr(\xv, y) \diff \xv \diff y \\
    & = \int \left( \int \ell(y, h(\xv)) \Pr(y|\xv) \diff y \right) \Pr(\xv) \diff \xv \\
    & = \Ebb_{\xv} \left[ \int \ell(y, h(\xv)) \Pr(y|\xv) \diff y \right] = \Ebb_{\xv} [ \Ebb_y [\ell(y, h(\xv)) | \xv]]
\end{align*}
$$

<div class="top-4"></div>

最小泛化风险称为{==贝叶斯风险==}，对应的$h^\star$即{==贝叶斯最优学习器==}

$$
\begin{align*}
    \qquad h^\star(\xv) = \mathop{\arg\min}_{h(\xv)} \int \ell(y, h(\xv)) \Pr(y|\xv) \diff y
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 贝叶斯最优学习器

---

<div class="top2"></div>

$$
\begin{align*}
    \qquad h^\star(\xv) = \mathop{\arg\min}_{h(\xv)} \int \ell(y, h(\xv)) \Pr(y|\xv) \diff y
\end{align*}
$$

<div class="top-4"></div>

回归问题通常采用平方损失$\ell(y, h(\xv)) = (y - h(\xv))^2$

$$
\begin{align*}
    \qquad \nabla_{h(\xv)} \left( \int (y - h(\xv))^2 \Pr(y|\xv) \diff y \right) & = 2 \int (h(\xv) - y) \Pr(y|\xv) \diff y \\
    & = 2 h(\xv) - 2 \int y \Pr(y|\xv) \diff y \\
    & = 2 h(\xv) - 2 \Ebb[y|\xv]
\end{align*}
$$

<div class="top-4"></div>

即回归问题的贝叶斯最优模型$h^\star(\xv) = \Ebb[y|\xv]$

我的批注 在偏差方差分解中我们曾得到相同的结论

$$
\begin{align*}
    \qquad \Ebb_{(\xv,y) \sim \Dcal} [(y - h(\xv))^2] = \Ebb_{\xv} [(h(\xv) - \Ebb [y|\xv])^2] + \noise
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 贝叶斯最优学习器

---

<div class="top2"></div>

$$
\begin{align*}
    \qquad h^\star(\xv) = \mathop{\arg\min}_{h(\xv)} \int \ell(y, h(\xv)) \Pr(y|\xv) \diff y
\end{align*}
$$

<div class="top-4"></div>

对分类问题，设$\Ycal = [c]$，$\ell(y, h(\xv)) = \Ibb(y \ne h(\xv))$，则

<div class="top1"></div>

$$
\begin{align*}
    \qquad h^\star(\xv) & = \mathop{\arg\min}_{\yhat \in [c]} \sum_{y \in [c]} \Ibb(y \ne \yhat) \Pr(y|\xv) \\[4pt]
    & = \mathop{\arg\min}_{\yhat \in [c]} ~ (1 - \Pr(\yhat|\xv)) \\[4pt]
    & = \mathop{\arg\max}_{\yhat \in [c]} ~ \Pr(\yhat|\xv)
\end{align*}
$$

<div class="top-4"></div>

即分类问题的贝叶斯最优模型$h^\star(\xv) = \mathop{\arg\max}_{\yhat \in [c]} \Pr(\yhat|\xv)$

我的批注 对于分类问题，{==泛化风险最小化==}等价于{==后验概率最大化==}

<!-- slide data-notes="" -->

##### 判别式 _vs._ 生成式

---

后验概率最大化的两种实现方式：判别式方法、生成式方法

判别式方法：直接用线性判别式拟合后验概率，如对率回归

$$
\begin{align*}
    \quad [\Pr(y=1|\xv), \ldots, \Pr(y=c|\xv)] = \softmax (\wv_1^\top \xv, \ldots, \wv_c^\top \xv)
\end{align*}
$$

生成式方法：曲线迂回，从数据的生成机制入手

$$
\begin{align*}
    \quad \overbrace{\Pr(y|\xv)}^{\text{后验}} & = \frac{\overbrace{\class{yellow}{\Pr(y)}}^{\text{先验}} \times \overbrace{\class{blue}{\Pr(\xv|y)}}^{\text{似然}}}{\Pr(\xv)}
    \Longrightarrow \begin{cases} \Pr(y = 1 | \xv) \propto \class{yellow}{\Pr(y=1)} \class{blue}{\Pr(\xv|y=1)} \\
    \qquad \vdots \\
    \Pr(y = c | \xv) \propto \class{yellow}{\Pr(y=c)} \class{blue}{\Pr(\xv|y=c)} \end{cases} \\[10pt]
    & \Longrightarrow \mathop{\arg \max}_{y \in [c]} \Pr(y|\xv) = \mathop{\arg \max}_{y \in [c]} \Pr(y) \Pr(\xv | y)
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 朴素贝叶斯

---

<div class="top2"></div>

$$
\begin{align*}
    \qquad \mathop{\arg \max}_{y \in [c]} \Pr(y|\xv) = \mathop{\arg \max}_{y \in [c]} \Pr(y) \Pr(\xv | y)
\end{align*}
$$

<div class="top-4"></div>

注意$\xv = [x_1; x_2; \ldots; x_d]$，于是似然

$$
\begin{align*}
    \qquad \Pr(\xv | y) = \Pr(x_1 | y) \cdot \Pr(x_2 | x_1, y) \cdots \Pr(x_d | x_{d-1}, \ldots, x_2, x_1, y)
\end{align*}
$$

<div class="top-3"></div>

当特征数$d$很大时似然会很难计算 (要考虑所有特征的所有取值)

<div class="top4"></div>

朴素贝叶斯 (<u>n</u>aïve <u>B</u>ayes, NB) 引入{==条件独立性假设==}：

$$
\begin{align*}
    \qquad \Pr(\xv | y) = \Pr(x_1 | y) \cdot \Pr(x_2 | y) \cdots \Pr(x_d | y) = \prod_{j \in [d]} \Pr(x_j | y)
\end{align*}
$$

<div class="top-2"></div>

问题：如何用数据集估计$\Pr(y), ~ \Pr(x_1 | y), ~ \Pr(x_2 | y), ~ \ldots, ~ \Pr(x_d | y)$
