---
presentation:
  margin: 0
  center: false
  transition: "convex"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"
@import "https://cdn.bootcdn.net/ajax/libs/jquery/3.5.0/jquery.js"

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## k-近邻法

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide vertical=true data-notes="" -->

##### 大纲

---

@import "../vega/outline.json" {as="vega" .top-2}

<!-- slide data-notes="" -->

##### <span style="font-weight:900">k-</span>近邻算法

---

基本假设：{==相似的样本属于相同的类别==}

<div class="top-2"></div>

如何刻画相似？距离函数：$\dist(\cdot, \cdot): \Xcal \times \Xcal \mapsto \Rbb^+$

<div class="top-2"></div>

集合与距离函数对$(\Xcal, \dist(\cdot, \cdot))$构成度量空间 (metric space)

<div class="top2"></div>

输入：$D = \{ (\xv_i, y_i) \}_{i \in [m]} \in (\Xcal \times \Ycal)^m$，近邻数$k$，待预测样本$\xv$

<div class="top-3"></div>

输出：$\xv$的类别$y$

1. 求解$N_k(\xv) \subseteq D$使得$|N_k(\xv)| = k$且对$\forall (\xv', y') \in D \setminus N_k(\xv)$有$\dist(\xv, \xv') \geq \max_{\zv \in N_k(\xv)} \dist (\xv, \zv)$
2. 输出$\mode(\{ y'': (\xv'', y'') \in N_k(\xv) \})$，其中$\mode(\cdot)$表示众数

<div class="top4"></div>

我的批注 近邻法没有{==显式==}的学习过程

<!-- slide vertical=true data-notes="" -->

##### 空间划分 <span style="font-weight:900">1-</span>近邻

---

@import "../tikz/knn.svg" {.center .width50 .top5}

<!-- slide vertical data-notes="" -->

##### 超参设置

---

近邻数$k$：取值范围$[m] \wedge \{2 \Zbb + 1\}$

- 奇数可保证取众数时不会出现打平的情况，zyzzj常委都是奇数位
- 越小越容易过拟合，越大越容易欠拟合，实操时可通过交叉验证选取

<div class="top2"></div>

距离函数：

- 闵可夫斯基距离：$\dist(\xv, \zv) = \| \xv - \zv \|_p$
- 马氏距离：$\dist_\Mv (\xv, \zv) \triangleq \sqrt{(\xv - \zv)^\top \Mv (\xv - \zv)}$，比如$\Mv$为对角阵$\diag\{w_1, \ldots, w_d\}$时，马氏距离就是加权平方距离$\sqrt{\sum_{j \in [d]} w_j (x_j - z_j)^2}$

<div class="top2"></div>

度量学习 (metric learning)：找一个更好的特征空间，记$\Mcal$为同类样本对集合，$\Ccal$为异类样本对集合

$$
\begin{align*}
    \quad \min_\Mv & \sum_{(\xv_i, \xv_j) \in \Mcal} \dist(\xv_i, \xv_j), \quad \st \sum_{(\xv_i, \xv_j) \in \Ccal} \dist(\xv_i, \xv_j) \ge 1, ~ \Mv \succeq \zerov
\end{align*}
$$

<!-- slide vertical data-notes="" -->

##### 优劣

---

优点

- 简单 (概念上、实现上)
- 无训练过程，只需存下数据，惰性学习 (lazy learning)
- 样本极少时也能用
- $\Xcal$维度不高时效果很好
- {==一致性==}：若贝叶斯最优分类器的错误率$R^\star = 0$，k-近邻也能渐进达到

<div class="top4"></div>

缺点

- 预测很慢，要计算待预测样本与训练集中所有样本的距离
- {==维度灾难==}：高维空间中的距离会失效，k-近邻效果很差

<!-- slide data-notes="" -->

##### 理论分析

---

{==渐进==}泛化错误率：当样本数$m \rightarrow \infty$时的泛化错误率

{==可分==} (separable) 度量空间$(\Xcal, \dist(\cdot, \cdot))$

空间 = 集合 + 结构约束

结构约束：加法和数乘的8条公理，线性空间

线性空间无法计算两个元素的距离，从而无法谈序列的收敛性

如何刻画元素间的距离？

- 邻域 (开集族)：拓扑空间
- 距离：度量空间
- 内积：内积空间，$\| \xv - \zv \|_2^2 = \langle \xv, \xv \rangle - 2 \langle \xv, \zv \rangle + \langle \zv, \zv \rangle$，诱导前者

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">1-</span>近邻算法 理论分析

---

设贝叶斯最优分类器$h^\star(\xv) = \mathop{\arg\max}_{y \in \Ycal} \Pbb(y|\xv)$的错误率为$R^\star$

$$
\begin{align*}
    \quad R^\star = \int \underbrace{(1 - \Pbb(h^\star(\xv) | \xv))}_{h^\star(\xv)\text{出错的概率}\qquad} \Pbb(\xv) \diff \xv = \int \Pbb (e^\star | \xv) \Pbb(\xv) \diff \xv
\end{align*}
$$

<div class="top-2"></div>

设1-近邻算法的错误率为$R_m$，其中下标$m$表示样本数，则

$$
\begin{align*}
    \quad R^\star \le R = \lim_{m \rightarrow \infty} R_m \le R^\star \left( 2 - \frac{c}{c-1} R^\star \right) \overset{c=2}{=} 2 R^\star (1 - R^\star)
\end{align*}
$$

对二分类问题，易知当$R^\star \in \{ 0, 1/2 \}$时，界是紧的

下界是显然的，下面考虑上界

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">1-</span>近邻算法 理论分析

---

设1-近邻算法的错误率为$R_m$，其中下标$m$表示样本数，则

$$
\begin{align*}
    \quad R^\star & = \int \underbrace{(1 - \Pbb(h^\star(\xv) | \xv))}_{h^\star(\xv)\text{出错的概率}\qquad} \Pbb(\xv) \diff \xv = \int \Pbb (e^\star | \xv) \Pbb(\xv) \diff \xv \\
    R^\star & \le R = \lim_{m \rightarrow \infty} R_m \le R^\star \left( 2 - \frac{c}{c-1} R^\star \right) \overset{c=2}{=} 2 R^\star (1 - R^\star)
\end{align*}
$$

<div class="top-2"></div>

预测出错的概率为$\Pbb(e|\xv) = 1 - \sum_{j \in [c]} \Pbb(y=j|\xv) \Pbb(\yhat=j|\xvhat)$，当$m \rightarrow \infty$时有$\xvhat \rightarrow \xv$，从而$\Pbb(e|\xv) = 1 - \sum_{j \in [c]} \Pbb(j|\xv)^2$，于是

$$
\begin{align*}
    \quad R = \int \Pbb(e|\xv) \Pbb(\xv) \diff \xv = \int \left( 1 - \sum_{j \in [c]} \Pbb(j|\xv)^2 \right) \Pbb(\xv) \diff \xv
\end{align*}
$$

<div class="top-2"></div>

欲证$R$的上界，需考虑$\sum_{j \in [c]} \Pbb(j|\xv)^2$的下界

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">1-</span>近邻算法 理论分析

---

<div class="top1"></div>

$$
\begin{align*}
    \quad R = \int \Pbb(e|\xv) \Pbb(\xv) \diff \xv = \int \left( 1 - \sum_{j \in [c]} \Pbb(j|\xv)^2 \right) \Pbb(\xv) \diff \xv
\end{align*}
$$

<div class="top-2"></div>

下面考虑$\sum_{j \in [c]} \Pbb(j|\xv)^2$的下界，由柯西不等式有

$$
\begin{align*}
    \quad \sum_{j \neq h^\star(\xv)} & \Pbb(j|\xv)^2 \geq \frac{( \sum_{j \neq h^\star(\xv)} \Pbb(j|\xv) )^2}{c-1}  = \frac{(1 - \Pbb(h^\star(\xv)|\xv))^2}{c-1} = \frac{\Pbb(e^\star|\xv)^2}{c-1} \\[5pt]
    \Longrightarrow 1 & - \sum_{j \in [c]} \Pbb(j|\xv)^2 = 1 - \Pbb(h^\star(\xv)|\xv)^2 - \sum_{j \neq h^\star(\xv)} \Pbb(j|\xv)^2 \\
    & \le 1 - (1 - \Pbb(e^\star|\xv))^2 - \frac{\Pbb(e^\star|\xv)^2}{c-1} \\
    & = 2 \Pbb(e^\star|\xv) - \Pbb(e^\star|\xv)^2 - \frac{\Pbb(e^\star|\xv)^2}{c-1} = 2 \Pbb(e^\star|\xv) - \frac{c}{c-1} \Pbb(e^\star|\xv)^2
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">1-</span>近邻算法 理论分析

---

将$\sum_{j \in [c]} \Pbb(j|\xv)^2$的下界回代有

$$
\begin{align*}
    \quad R & = \int \Pbb(e|\xv) \Pbb(\xv) \diff \xv = \int \left( 1 - \sum_{j \in [c]} \Pbb(j|\xv)^2 \right) \Pbb(\xv) \diff \xv \\
    & \le \int \left( 2 \Pbb(e^\star|\xv) - \frac{c}{c-1} \Pbb(e^\star|\xv)^2 \right) \Pbb(\xv) \diff \xv \\
    & = 2 R^\star - \frac{c}{c-1} \int \Pbb(e^\star|\xv)^2 \Pbb(\xv) \diff \xv \quad \longleftarrow \text{第二项是二阶矩} \\
    & = 2 R^\star - \frac{c}{c-1} ({R^\star}^2 + \text{方差}) \\
    & \le R^\star \left( 2 - \frac{c}{c-1} R^\star \right) 
\end{align*}
$$

<!-- slide data-notes="" -->

##### <span style="font-weight:900">k-</span>近邻算法

---

输入：$D = \{ (\xv_i, y_i) \}_{i \in [m]} \in (\Xcal \times \Ycal)^m$，待预测样本$\xv$

<div class="top-3"></div>

输出：$\xv$的类别$y$

1. 根据选定的距离度量，记$D$中与$\xv$最近的$k$个样本构成的集合为$N_k(\xv)$
2. 在$N_k(\xv)$中根据{==多数表决==}确定$\xv$的类别$y$

<div class="top2"></div>

$$
\begin{align*}
    \qquad y = \mathop{\arg \max}_{\yhat \in [c]} \sum_{\xv_i \in N_k(\xv)} \Ibb(y_i = \yhat)
\end{align*}
$$

一些说明：

- $k$选择奇数，避免出现打平的情况，中央政治局常委也都是奇数位
- 当$m \rightarrow \infty$时有$R^\star \le \cdots \le R^{(5)} \le R^{(3)} \le R^{(1)} \le 2 R^\star (1 - R^\star)$
- 在$m$有限的情况下，$k$并不是越大越好，取$k = m$往往欠拟合
- 一些改进的变种：加权多数表决，带拒绝的多数表决

<!-- slide data-notes="" -->

##### 维度灾难

---

