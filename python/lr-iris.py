import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = LogisticRegression(penalty='none', verbose=True)  # 无正则项 输出日志
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))
# 1.0

print(OneHotEncoder().fit_transform(y_test[:, np.newaxis]).toarray())
# [[0. 1. 0.]
#  [1. 0. 0.]
#  [0. 0. 1.]
#  [0. 1. 0.]
#  [0. 1. 0.]
#  [1. 0. 0.]
#  [0. 1. 0.]
#  [0. 0. 1.]
#  [0. 1. 0.]
#  [0. 1. 0.]
#  [0. 0. 1.]
#  [1. 0. 0.]
#  [1. 0. 0.]
#  [1. 0. 0.]
#  [1. 0. 0.]
#  [0. 1. 0.]
#  [0. 0. 1.]
#  [0. 1. 0.]
#  [0. 1. 0.]
#  [0. 0. 1.]
#  [1. 0. 0.]
#  [0. 0. 1.]
#  [1. 0. 0.]
#  [0. 0. 1.]
#  [0. 0. 1.]
#  [0. 0. 1.]
#  [0. 0. 1.]
#  [0. 0. 1.]
#  [1. 0. 0.]
#  [1. 0. 0.]]

print(clf.predict_proba(X_test))
# [[1.88995213e-15 9.99887032e-01 1.12968186e-04]
#  [1.00000000e+00 1.06030446e-13 1.61146012e-37]
#  [2.34667797e-41 1.07023162e-11 1.00000000e+00]
#  [6.21159520e-14 9.98139517e-01 1.86048274e-03]
#  [1.00449599e-14 9.98435547e-01 1.56445297e-03]
#  [1.00000000e+00 1.98333658e-12 8.75409602e-35]
#  [7.58010355e-08 9.99999832e-01 9.18241192e-08]
#  [8.36463589e-20 1.88306452e-04 9.99811694e-01]
#  [5.27298441e-17 8.70853181e-01 1.29146819e-01]
#  [1.45094379e-10 9.99999448e-01 5.51510245e-07]
#  [6.28254238e-18 2.17459760e-02 9.78254024e-01]
#  [1.00000000e+00 4.89303790e-10 3.77427415e-33]
#  [1.00000000e+00 7.22045678e-15 9.14311735e-40]
#  [9.99999999e-01 5.19318165e-10 3.74068137e-33]
#  [1.00000000e+00 8.49922121e-14 8.81638050e-38]
#  [7.71165641e-13 9.98276490e-01 1.72351024e-03]
#  [5.56328926e-29 6.23226675e-07 9.99999377e-01]
#  [5.99786952e-12 9.99999357e-01 6.42707897e-07]
#  [5.74797663e-15 9.99739046e-01 2.60953590e-04]
#  [1.17027774e-28 6.72720611e-07 9.99999327e-01]
#  [9.99999996e-01 3.50352401e-09 2.31821675e-31]
#  [4.16008331e-17 2.53905554e-01 7.46094446e-01]
#  [1.00000000e+00 1.02762174e-10 2.44475510e-32]
#  [7.45084352e-28 3.48907882e-06 9.99996511e-01]
#  [4.11711541e-23 5.49426688e-04 9.99450573e-01]
#  [2.19606769e-22 2.69569986e-05 9.99973043e-01]
#  [4.37496068e-29 2.23589072e-05 9.99977641e-01]
#  [2.52950299e-28 3.91596903e-07 9.99999608e-01]
#  [9.99999999e-01 7.37325887e-10 1.52992058e-31]
#  [9.99999994e-01 6.37873593e-09 6.77211243e-31]]
